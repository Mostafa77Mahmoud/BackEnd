{"file_contents":{"BACKEND_DOCUMENTATION.md":{"content":"\r\n# Shariaa Contract Analyzer Backend Documentation\r\n\r\n## Table of Contents\r\n\r\n1. [System Overview](#system-overview)\r\n2. [Architecture](#architecture)\r\n3. [Technology Stack](#technology-stack)\r\n4. [Core Components](#core-components)\r\n5. [API Endpoints](#api-endpoints)\r\n6. [Database Schema](#database-schema)\r\n7. [File Processing Pipeline](#file-processing-pipeline)\r\n8. [Configuration Management](#configuration-management)\r\n9. [Error Handling & Logging](#error-handling--logging)\r\n10. [Security Considerations](#security-considerations)\r\n11. [Deployment Architecture](#deployment-architecture)\r\n12. [Performance Optimization](#performance-optimization)\r\n13. [Monitoring & Maintenance](#monitoring--maintenance)\r\n\r\n## System Overview\r\n\r\nThe Shariaa Contract Analyzer is a sophisticated backend system designed to analyze legal contracts for compliance with Islamic law (Sharia) principles, specifically following AAOIFI (Accounting and Auditing Organization for Islamic Financial Institutions) standards. The system leverages advanced AI capabilities through Google's Generative AI models to provide intelligent contract analysis, modification suggestions, and expert review capabilities.\r\n\r\n### Key Features\r\n\r\n- **Multi-format Contract Processing**: Supports DOCX, PDF, and TXT formats\r\n- **AI-Powered Sharia Compliance Analysis**: Uses Google Gemini 2.0 Flash for intelligent analysis\r\n- **Interactive User Consultation**: Real-time Q&A about contract terms\r\n- **Expert Review System**: Professional expert feedback integration\r\n- **Contract Modification**: Automated generation of compliant contract versions\r\n- **Document Management**: Cloud-based storage with Cloudinary integration\r\n- **Multi-language Support**: Primarily Arabic with English support\r\n\r\n## Architecture\r\n\r\n### High-Level Architecture Diagram\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"Client Layer\"\r\n        WEB[Web Client]\r\n        MOBILE[Mobile App]\r\n    end\r\n    \r\n    subgraph \"API Gateway\"\r\n        FLASK[Flask Server<br/>Port 5000]\r\n    end\r\n    \r\n    subgraph \"Core Services\"\r\n        AUTH[Authentication<br/>Session Management]\r\n        ANALYZER[Contract Analyzer<br/>Service]\r\n        PROCESSOR[Document Processor]\r\n        GENERATOR[Contract Generator]\r\n    end\r\n    \r\n    subgraph \"AI Services\"\r\n        GEMINI[Google Gemini AI<br/>Text Analysis]\r\n        EXTRACT[Document Extraction<br/>AI Service]\r\n    end\r\n    \r\n    subgraph \"Storage Layer\"\r\n        MONGO[(MongoDB Atlas<br/>Contract Data)]\r\n        CLOUDINARY[Cloudinary<br/>File Storage]\r\n        TEMP[Temporary Storage<br/>Local Processing]\r\n    end\r\n    \r\n    subgraph \"External Libraries\"\r\n        DOCX[python-docx<br/>Document Processing]\r\n        LIBRE[LibreOffice<br/>PDF Conversion]\r\n    end\r\n    \r\n    WEB --> FLASK\r\n    MOBILE --> FLASK\r\n    FLASK --> AUTH\r\n    FLASK --> ANALYZER\r\n    FLASK --> PROCESSOR\r\n    FLASK --> GENERATOR\r\n    \r\n    ANALYZER --> GEMINI\r\n    PROCESSOR --> EXTRACT\r\n    PROCESSOR --> DOCX\r\n    PROCESSOR --> LIBRE\r\n    \r\n    FLASK --> MONGO\r\n    FLASK --> CLOUDINARY\r\n    PROCESSOR --> TEMP\r\n    \r\n    GEMINI --> ANALYZER\r\n    EXTRACT --> PROCESSOR\r\n```\r\n\r\n### Data Flow Architecture\r\n\r\n```mermaid\r\nsequenceDiagram\r\n    participant Client\r\n    participant Flask\r\n    participant Processor\r\n    participant AI\r\n    participant Storage\r\n    participant Generator\r\n    \r\n    Client->>Flask: Upload Contract\r\n    Flask->>Storage: Store Original File\r\n    Flask->>Processor: Extract Text\r\n    Processor->>AI: Send for Analysis\r\n    AI-->>Processor: Analysis Results\r\n    Processor->>Storage: Store Analysis\r\n    Storage-->>Flask: Confirmation\r\n    Flask-->>Client: Analysis Response\r\n    \r\n    Client->>Flask: Request Modifications\r\n    Flask->>Generator: Generate Modified Contract\r\n    Generator->>Storage: Store Modified Files\r\n    Storage-->>Flask: File URLs\r\n    Flask-->>Client: Download Links\r\n```\r\n\r\n## Technology Stack\r\n\r\n### Core Framework\r\n- **Flask 2.3+**: Python web framework for RESTful API\r\n- **Python 3.12**: Primary programming language\r\n- **WSGI**: Web Server Gateway Interface\r\n\r\n### AI & Machine Learning\r\n- **Google Generative AI**: Gemini 2.0 Flash Thinking model\r\n- **google-generativeai**: Python SDK for Google AI services\r\n- **Temperature Control**: Configurable AI response variability\r\n\r\n### Document Processing\r\n- **python-docx**: Microsoft Word document manipulation\r\n- **LibreOffice**: PDF conversion and document processing\r\n- **unidecode**: Unicode transliteration for filename safety\r\n- **langdetect**: Automatic language detection\r\n\r\n### Database & Storage\r\n- **MongoDB Atlas**: Primary database for contract and term storage\r\n- **PyMongo**: MongoDB Python driver\r\n- **Cloudinary**: Cloud-based file storage and management\r\n- **Temporary Storage**: Local file system for processing\r\n\r\n### Security & Validation\r\n- **Flask-CORS**: Cross-Origin Resource Sharing\r\n- **Werkzeug**: Security utilities and file handling\r\n- **Input Validation**: Custom validation for all endpoints\r\n\r\n### Utilities\r\n- **requests**: HTTP client for external API calls\r\n- **pathlib**: Modern path handling\r\n- **traceback**: Error tracking and debugging\r\n- **datetime**: Timezone-aware timestamp management\r\n\r\n## Core Components\r\n\r\n### 1. API Server (`api_server.py`)\r\n\r\nThe main Flask application that orchestrates all backend operations.\r\n\r\n**Key Responsibilities:**\r\n- HTTP request handling and routing\r\n- Session management and user state\r\n- Integration with external services\r\n- Response formatting and error handling\r\n\r\n**Critical Functions:**\r\n```python\r\n@app.route(\"/analyze\", methods=[\"POST\"])\r\ndef analyze_file():\r\n    \"\"\"\r\n    Main contract analysis endpoint\r\n    - Accepts multi-format file uploads\r\n    - Processes documents through AI pipeline\r\n    - Stores results in database\r\n    - Returns structured analysis\r\n    \"\"\"\r\n\r\n@app.route(\"/generate_modified_contract\", methods=[\"POST\"])\r\ndef generate_modified_contract():\r\n    \"\"\"\r\n    Generates modified compliant contracts\r\n    - Applies user-confirmed modifications\r\n    - Creates DOCX and TXT versions\r\n    - Uploads to cloud storage\r\n    \"\"\"\r\n```\r\n\r\n### 2. Document Processing (`doc_processing.py`)\r\n\r\nSophisticated document manipulation and conversion system.\r\n\r\n**Core Capabilities:**\r\n- **Text Extraction**: Converts DOCX to structured markdown with ID preservation\r\n- **Document Generation**: Creates professional DOCX files with Arabic RTL support\r\n- **PDF Conversion**: LibreOffice integration for PDF generation\r\n- **Formatting Preservation**: Maintains bold, italic, underline, and table structures\r\n\r\n**Processing Pipeline:**\r\n```python\r\ndef build_structured_text_for_analysis(doc: DocxDocument) -> tuple[str, str]:\r\n    \"\"\"\r\n    Extracts text with unique paragraph/table IDs\r\n    Returns: (structured_markdown, plain_text)\r\n    \"\"\"\r\n\r\ndef create_docx_from_llm_markdown(\r\n    original_markdown_text: str,\r\n    output_path: str,\r\n    contract_language: str = 'ar',\r\n    terms_for_marking: list[dict] | dict | None = None\r\n):\r\n    \"\"\"\r\n    Creates professional DOCX with term highlighting\r\n    Supports Arabic RTL layout and color coding\r\n    \"\"\"\r\n```\r\n\r\n### 3. Remote API Integration (`remote_api.py`)\r\n\r\nManages all interactions with Google's Generative AI services.\r\n\r\n**Features:**\r\n- **Session Management**: Persistent chat sessions for context\r\n- **File Processing**: Direct AI-based text extraction from PDFs\r\n- **Error Handling**: Robust retry logic and safety filtering\r\n- **Response Cleaning**: Intelligent JSON extraction from AI responses\r\n\r\n### 4. Utility Functions (`utils.py`)\r\n\r\nEssential helper functions for file operations and data processing.\r\n\r\n**Key Utilities:**\r\n- **Filename Sanitization**: Arabic text transliteration and safety\r\n- **Cloud Storage**: Cloudinary upload helpers\r\n- **Response Cleaning**: AI response parsing and validation\r\n\r\n## API Endpoints\r\n\r\n### Contract Analysis Endpoints\r\n\r\n#### POST `/analyze`\r\n**Purpose**: Upload and analyze contracts for Sharia compliance\r\n\r\n**Request Format:**\r\n```http\r\nPOST /analyze\r\nContent-Type: multipart/form-data\r\n\r\nfile: [Contract file - DOCX/PDF/TXT]\r\n```\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"message\": \"Contract analyzed successfully.\",\r\n    \"analysis_results\": [\r\n        {\r\n            \"term_id\": \"clause_1\",\r\n            \"term_text\": \"Contract clause text...\",\r\n            \"is_valid_sharia\": false,\r\n            \"sharia_issue\": \"Interest-based transaction detected\",\r\n            \"reference_number\": \"AAOIFI Standard 5\",\r\n            \"modified_term\": \"Suggested compliant alternative...\"\r\n        }\r\n    ],\r\n    \"session_id\": \"uuid-session-identifier\",\r\n    \"detected_contract_language\": \"ar\",\r\n    \"original_cloudinary_url\": \"https://cloudinary.com/...\"\r\n}\r\n```\r\n\r\n#### GET `/terms/<session_id>`\r\n**Purpose**: Retrieve all analyzed terms for a session\r\n\r\n**Response Format:**\r\n```json\r\n[\r\n    {\r\n        \"_id\": \"mongodb-object-id\",\r\n        \"session_id\": \"session-uuid\",\r\n        \"term_id\": \"clause_1\",\r\n        \"term_text\": \"Original clause text\",\r\n        \"is_valid_sharia\": false,\r\n        \"sharia_issue\": \"Compliance issue description\",\r\n        \"modified_term\": \"Suggested modification\",\r\n        \"is_confirmed_by_user\": true,\r\n        \"confirmed_modified_text\": \"User-approved text\"\r\n    }\r\n]\r\n```\r\n\r\n### Contract Generation Endpoints\r\n\r\n#### POST `/generate_modified_contract`\r\n**Purpose**: Generate compliant contract versions\r\n\r\n**Request Format:**\r\n```json\r\n{\r\n    \"session_id\": \"session-uuid\"\r\n}\r\n```\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"success\": true,\r\n    \"message\": \"Modified contract generated.\",\r\n    \"modified_docx_cloudinary_url\": \"https://cloudinary.com/docx\",\r\n    \"modified_txt_cloudinary_url\": \"https://cloudinary.com/txt\"\r\n}\r\n```\r\n\r\n#### POST `/generate_marked_contract`\r\n**Purpose**: Generate contract with highlighted terms\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"success\": true,\r\n    \"message\": \"Marked contract generated.\",\r\n    \"marked_docx_cloudinary_url\": \"https://cloudinary.com/marked.docx\"\r\n}\r\n```\r\n\r\n### Interactive Consultation Endpoints\r\n\r\n#### POST `/interact`\r\n**Purpose**: Real-time Q&A about contract terms\r\n\r\n**Request Format:**\r\n```json\r\n{\r\n    \"question\": \"User question about contract\",\r\n    \"term_id\": \"clause_1\",\r\n    \"term_text\": \"Specific clause text\",\r\n    \"session_id\": \"session-uuid\"\r\n}\r\n```\r\n\r\n**Response**: Plain text response from AI consultant\r\n\r\n#### POST `/review_modification`\r\n**Purpose**: Expert review of user modifications\r\n\r\n**Request Format:**\r\n```json\r\n{\r\n    \"term_id\": \"clause_1\",\r\n    \"user_modified_text\": \"User's proposed changes\",\r\n    \"original_term_text\": \"Original clause text\",\r\n    \"session_id\": \"session-uuid\"\r\n}\r\n```\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"reviewed_text\": \"Expert-reviewed text\",\r\n    \"is_still_valid_sharia\": true,\r\n    \"new_sharia_issue\": null,\r\n    \"new_reference_number\": null\r\n}\r\n```\r\n\r\n### Document Preview Endpoints\r\n\r\n#### GET `/preview_contract/<session_id>/<contract_type>`\r\n**Purpose**: Generate PDF previews of contracts\r\n\r\n**Parameters:**\r\n- `contract_type`: \"modified\" or \"marked\"\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"pdf_url\": \"https://cloudinary.com/preview.pdf\"\r\n}\r\n```\r\n\r\n#### GET `/download_pdf_preview/<session_id>/<contract_type>`\r\n**Purpose**: Direct PDF download proxy\r\n\r\n**Response**: Binary PDF stream with appropriate headers\r\n\r\n### Expert Feedback System\r\n\r\n#### POST `/feedback/expert`\r\n**Purpose**: Submit expert evaluation of AI analysis\r\n\r\n**Request Format:**\r\n```json\r\n{\r\n    \"session_id\": \"session-uuid\",\r\n    \"term_id\": \"clause_1\",\r\n    \"feedback_data\": {\r\n        \"aiAnalysisApproved\": false,\r\n        \"expertIsValidSharia\": true,\r\n        \"expertComment\": \"Expert analysis...\",\r\n        \"expertCorrectedShariaIssue\": \"Corrected issue\",\r\n        \"expertCorrectedReference\": \"Correct AAOIFI reference\",\r\n        \"expertCorrectedSuggestion\": \"Expert suggestion\"\r\n    }\r\n}\r\n```\r\n\r\n### Statistics and History\r\n\r\n#### GET `/api/stats/user`\r\n**Purpose**: User analytics and statistics\r\n\r\n**Response Format:**\r\n```json\r\n{\r\n    \"totalSessions\": 25,\r\n    \"totalTerms\": 150,\r\n    \"complianceRate\": 78.5,\r\n    \"averageProcessingTime\": 15.5\r\n}\r\n```\r\n\r\n#### GET `/api/history`\r\n**Purpose**: Complete analysis history with enriched data\r\n\r\n**Response**: Array of session objects with calculated metrics\r\n\r\n## Database Schema\r\n\r\n### MongoDB Collections\r\n\r\n#### 1. `contracts` Collection\r\n**Purpose**: Main contract session storage\r\n\r\n```javascript\r\n{\r\n    _id: ObjectId | String,\r\n    session_id: String,\r\n    original_filename: String,\r\n    original_cloudinary_info: {\r\n        url: String,\r\n        public_id: String,\r\n        format: String,\r\n        user_facing_filename: String\r\n    },\r\n    analysis_results_cloudinary_info: Object,\r\n    original_format: String, // \"docx\", \"pdf\", \"txt\"\r\n    original_contract_plain: String,\r\n    original_contract_markdown: String,\r\n    generated_markdown_from_docx: String,\r\n    detected_contract_language: String, // \"ar\" or \"en\"\r\n    analysis_timestamp: Date,\r\n    confirmed_terms: {\r\n        [term_id]: {\r\n            original_text: String,\r\n            confirmed_text: String\r\n        }\r\n    },\r\n    interactions: [{\r\n        user_question: String,\r\n        term_id: String,\r\n        term_text: String,\r\n        response: String,\r\n        timestamp: Date\r\n    }],\r\n    modified_contract_info: {\r\n        docx_cloudinary_info: Object,\r\n        txt_cloudinary_info: Object,\r\n        generation_timestamp: String\r\n    },\r\n    marked_contract_info: {\r\n        docx_cloudinary_info: Object,\r\n        generation_timestamp: String\r\n    },\r\n    pdf_preview_info: {\r\n        modified: Object,\r\n        marked: Object\r\n    }\r\n}\r\n```\r\n\r\n#### 2. `terms` Collection\r\n**Purpose**: Individual term analysis storage\r\n\r\n```javascript\r\n{\r\n    _id: ObjectId,\r\n    session_id: String,\r\n    term_id: String,\r\n    term_text: String,\r\n    is_valid_sharia: Boolean,\r\n    sharia_issue: String | null,\r\n    reference_number: String | null,\r\n    modified_term: String | null,\r\n    is_confirmed_by_user: Boolean,\r\n    confirmed_modified_text: String,\r\n    has_expert_feedback: Boolean,\r\n    last_expert_feedback_id: ObjectId,\r\n    expert_override_is_valid_sharia: Boolean\r\n}\r\n```\r\n\r\n#### 3. `expert_feedback` Collection\r\n**Purpose**: Expert review and validation\r\n\r\n```javascript\r\n{\r\n    _id: ObjectId,\r\n    session_id: String,\r\n    term_id: String,\r\n    original_term_text_snapshot: String,\r\n    expert_user_id: String,\r\n    expert_username: String,\r\n    feedback_timestamp: Date,\r\n    ai_initial_analysis_assessment: {\r\n        is_correct_compliance: Boolean\r\n    },\r\n    expert_verdict_is_valid_sharia: Boolean,\r\n    expert_comment_on_term: String,\r\n    expert_corrected_sharia_issue: String,\r\n    expert_corrected_reference: String,\r\n    expert_final_suggestion_for_term: String,\r\n    // Snapshot of original AI analysis\r\n    original_ai_is_valid_sharia: Boolean,\r\n    original_ai_sharia_issue: String,\r\n    original_ai_modified_term: String,\r\n    original_ai_reference_number: String\r\n}\r\n```\r\n\r\n## File Processing Pipeline\r\n\r\n### Document Processing Flow\r\n\r\n```mermaid\r\ngraph TD\r\n    A[File Upload] --> B{File Type?}\r\n    B -->|DOCX| C[Extract with python-docx]\r\n    B -->|PDF| D[AI Text Extraction]\r\n    B -->|TXT| E[Direct Processing]\r\n    \r\n    C --> F[Generate Structured Markdown]\r\n    D --> G[Process Extracted Text]\r\n    E --> G\r\n    \r\n    F --> H[Language Detection]\r\n    G --> H\r\n    \r\n    H --> I[AI Analysis with Gemini]\r\n    I --> J[Parse JSON Results]\r\n    J --> K[Store in Database]\r\n    \r\n    K --> L[Upload to Cloudinary]\r\n    L --> M[Return Analysis Results]\r\n    \r\n    subgraph \"Structured Text Generation\"\r\n        F --> F1[Preserve Formatting]\r\n        F --> F2[Assign Unique IDs]\r\n        F --> F3[Table Processing]\r\n        F --> F4[Markdown Conversion]\r\n    end\r\n    \r\n    subgraph \"AI Processing\"\r\n        I --> I1[System Prompt Formatting]\r\n        I --> I2[Safety Settings]\r\n        I --> I3[Response Validation]\r\n        I --> I4[Retry Logic]\r\n    end\r\n```\r\n\r\n### Document Generation Pipeline\r\n\r\n```mermaid\r\ngraph TD\r\n    A[Modification Request] --> B[Retrieve Original Markdown]\r\n    B --> C[Apply Confirmed Changes]\r\n    C --> D[Generate DOCX]\r\n    C --> E[Generate TXT]\r\n    \r\n    D --> F[RTL Language Support]\r\n    F --> G[Professional Styling]\r\n    G --> H[Term Highlighting]\r\n    \r\n    H --> I[Upload to Cloudinary]\r\n    E --> I\r\n    \r\n    I --> J[PDF Conversion]\r\n    J --> K[LibreOffice Processing]\r\n    K --> L[Cloud Storage]\r\n    \r\n    subgraph \"DOCX Generation Features\"\r\n        G --> G1[Arabic Font Support]\r\n        G --> G2[Proper Alignment]\r\n        G --> G3[Color Coding]\r\n        G --> G4[Signature Blocks]\r\n    end\r\n    \r\n    subgraph \"PDF Processing\"\r\n        K --> K1[Headless Conversion]\r\n        K --> K2[Error Handling]\r\n        K --> K3[File Validation]\r\n    end\r\n```\r\n\r\n## Configuration Management\r\n\r\n### Environment Variables\r\n\r\nThe system uses a centralized configuration approach in `config.py`:\r\n\r\n```python\r\n# Cloud Storage Configuration\r\nCLOUDINARY_CLOUD_NAME = \"your-cloud-name\"\r\nCLOUDINARY_API_KEY = \"your-api-key\"\r\nCLOUDINARY_API_SECRET = \"your-api-secret\"\r\nCLOUDINARY_BASE_FOLDER = \"shariaa_analyzer_uploads\"\r\n\r\n# AI Service Configuration\r\nGOOGLE_API_KEY = \"your-google-api-key\"\r\nMODEL_NAME = \"gemini-2.0-flash-thinking-exp-01-21\"\r\nTEMPERATURE = 0  # Deterministic responses\r\n\r\n# Database Configuration\r\nMONGO_URI = \"mongodb+srv://username:password@cluster.mongodb.net/database\"\r\n\r\n# External Tools\r\nLIBREOFFICE_PATH = \"/path/to/libreoffice/soffice\"\r\n\r\n# Security\r\nFLASK_SECRET_KEY = \"your-secret-key\"\r\n```\r\n\r\n### Cloudinary Folder Structure\r\n\r\n```\r\nshariaa_analyzer_uploads/\r\n├── {session_id}/\r\n    ├── original_contracts/\r\n    │   └── original_file.{ext}\r\n    ├── analysis_results_json/\r\n    │   └── analysis_results.json\r\n    ├── modified_contracts/\r\n    │   ├── modified_contract.docx\r\n    │   └── modified_contract.txt\r\n    ├── marked_contracts/\r\n    │   └── marked_contract.docx\r\n    └── pdf_previews/\r\n        ├── modified_preview.pdf\r\n        └── marked_preview.pdf\r\n```\r\n\r\n## Error Handling & Logging\r\n\r\n### Logging Architecture\r\n\r\nThe system implements comprehensive logging across all components:\r\n\r\n```python\r\nimport logging\r\n\r\n# Configure centralized logging\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\r\n    handlers=[\r\n        logging.StreamHandler(),\r\n        logging.FileHandler('shariaa_analyzer.log', encoding='utf-8')\r\n    ]\r\n)\r\n```\r\n\r\n### Error Categories\r\n\r\n1. **Input Validation Errors**\r\n   - Invalid file formats\r\n   - Missing required parameters\r\n   - Malformed JSON requests\r\n\r\n2. **Processing Errors**\r\n   - AI service failures\r\n   - Document conversion issues\r\n   - Database connectivity problems\r\n\r\n3. **External Service Errors**\r\n   - Cloudinary upload failures\r\n   - MongoDB connection issues\r\n   - LibreOffice conversion errors\r\n\r\n### Error Response Format\r\n\r\n```json\r\n{\r\n    \"error\": \"Descriptive error message\",\r\n    \"error_code\": \"ERROR_CATEGORY_SPECIFIC\",\r\n    \"timestamp\": \"2024-01-15T10:30:00Z\",\r\n    \"session_id\": \"uuid-if-available\"\r\n}\r\n```\r\n\r\n## Security Considerations\r\n\r\n### Input Validation\r\n\r\n1. **File Upload Security**\r\n   - File type validation\r\n   - Size limitations (16MB max)\r\n   - Secure filename generation\r\n   - Virus scanning considerations\r\n\r\n2. **Data Sanitization**\r\n   - SQL injection prevention (MongoDB parameterized queries)\r\n   - XSS prevention in responses\r\n   - Path traversal protection\r\n\r\n### Authentication & Authorization\r\n\r\n1. **Session Management**\r\n   - Secure session cookies\r\n   - Session timeout implementation\r\n   - CSRF protection considerations\r\n\r\n2. **API Security**\r\n   - Rate limiting recommendations\r\n   - Input parameter validation\r\n   - Response data filtering\r\n\r\n### Data Protection\r\n\r\n1. **Sensitive Data Handling**\r\n   - Contract content encryption at rest\r\n   - Secure temporary file handling\r\n   - Automatic cleanup procedures\r\n\r\n2. **Privacy Compliance**\r\n   - Data retention policies\r\n   - User consent tracking\r\n   - Audit trail maintenance\r\n\r\n## Performance Optimization\r\n\r\n### Caching Strategy\r\n\r\n```mermaid\r\ngraph LR\r\n    A[Request] --> B{Cache Check}\r\n    B -->|Hit| C[Return Cached Result]\r\n    B -->|Miss| D[Process Request]\r\n    D --> E[Store in Cache]\r\n    E --> F[Return Result]\r\n    \r\n    subgraph \"Cache Layers\"\r\n        G[Session Cache]\r\n        H[Analysis Results Cache]\r\n        I[Generated Documents Cache]\r\n    end\r\n```\r\n\r\n### Database Optimization\r\n\r\n1. **Indexing Strategy**\r\n   ```javascript\r\n   // Recommended indexes\r\n   db.contracts.createIndex({ \"session_id\": 1 })\r\n   db.terms.createIndex({ \"session_id\": 1, \"term_id\": 1 })\r\n   db.expert_feedback.createIndex({ \"session_id\": 1, \"term_id\": 1 })\r\n   ```\r\n\r\n2. **Query Optimization**\r\n   - Efficient aggregation pipelines\r\n   - Projection optimization\r\n   - Connection pooling\r\n\r\n### File Processing Optimization\r\n\r\n1. **Temporary File Management**\r\n   - Automatic cleanup procedures\r\n   - Memory-efficient streaming\r\n   - Parallel processing capabilities\r\n\r\n2. **Cloud Storage Optimization**\r\n   - Optimized upload parameters\r\n   - CDN utilization\r\n   - Bandwidth management\r\n\r\n## Deployment Architecture\r\n\r\n### Replit Deployment Configuration\r\n\r\n```toml\r\n# .replit configuration\r\nmodules = [\"python-3.12\"]\r\nrun = \"python api_server.py\"\r\n\r\n[nix]\r\nchannel = \"stable-25_05\"\r\n\r\n[deployment]\r\nrun = [\"sh\", \"-c\", \"python api_server.py\"]\r\n\r\n[workflows]\r\nrunButton = \"Run Server\"\r\n\r\n[[workflows.workflow]]\r\nname = \"Run Server\"\r\nauthor = 46224424\r\nmode = \"sequential\"\r\n\r\n[[workflows.workflow.tasks]]\r\ntask = \"shell.exec\"\r\nargs = \"python api_server.py\"\r\n```\r\n\r\n### Production Considerations\r\n\r\n1. **Scalability**\r\n   - Horizontal scaling capabilities\r\n   - Load balancing recommendations\r\n   - Resource monitoring\r\n\r\n2. **Availability**\r\n   - Health check endpoints\r\n   - Graceful shutdown procedures\r\n   - Error recovery mechanisms\r\n\r\n### Environment Setup\r\n\r\n1. **Dependencies Management**\r\n   ```txt\r\n   # Key requirements.txt entries\r\n   Flask>=2.3.0\r\n   pymongo>=4.3.0\r\n   google-generativeai>=0.3.0\r\n   python-docx>=0.8.11\r\n   cloudinary>=1.34.0\r\n   flask-cors>=4.0.0\r\n   ```\r\n\r\n2. **System Dependencies**\r\n   - LibreOffice installation\r\n   - Python 3.12+ runtime\r\n   - UTF-8 locale support\r\n\r\n## Monitoring & Maintenance\r\n\r\n### Health Monitoring\r\n\r\n1. **Application Metrics**\r\n   - Request response times\r\n   - Error rates by endpoint\r\n   - Processing queue lengths\r\n   - Memory usage patterns\r\n\r\n2. **External Service Monitoring**\r\n   - AI service availability\r\n   - Database connection health\r\n   - Cloud storage accessibility\r\n\r\n### Maintenance Procedures\r\n\r\n1. **Regular Tasks**\r\n   - Log file rotation\r\n   - Temporary file cleanup\r\n   - Database optimization\r\n   - Security updates\r\n\r\n2. **Backup Strategies**\r\n   - Database backup procedures\r\n   - Configuration backup\r\n   - Disaster recovery planning\r\n\r\n### Performance Metrics\r\n\r\n```mermaid\r\ngraph TD\r\n    A[Performance Monitoring] --> B[Response Time Metrics]\r\n    A --> C[Error Rate Tracking]\r\n    A --> D[Resource Utilization]\r\n    \r\n    B --> B1[API Endpoint Times]\r\n    B --> B2[AI Processing Duration]\r\n    B --> B3[Document Generation Speed]\r\n    \r\n    C --> C1[HTTP Error Codes]\r\n    C --> C2[AI Service Failures]\r\n    C --> C3[Database Errors]\r\n    \r\n    D --> D1[Memory Usage]\r\n    D --> D2[CPU Utilization]\r\n    D --> D3[Storage Consumption]\r\n```\r\n\r\n## Modern Flask Architecture (Updated 2024)\n\n### Application Factory Pattern\n\nThe application has been restructured to use Flask's modern application factory pattern with organized blueprints:\n\n**Main Factory (`app/__init__.py`):**\n```python\ndef create_app(config_name='default'):\n    \"\"\"Creates and configures Flask application instance\"\"\"\n    app = Flask(__name__)\n    \n    # Load environment-based configuration\n    if config_name == 'production':\n        app.config.from_object('config.production.ProductionConfig')\n    else:\n        app.config.from_object('config.default.DefaultConfig')\n    \n    # Register blueprints for modular routing\n    from app.routes.analysis import analysis_bp\n    from app.routes.generation import generation_bp\n    from app.routes.interaction import interaction_bp\n    from app.routes.admin import admin_bp\n    \n    app.register_blueprint(analysis_bp, url_prefix='/api')\n    app.register_blueprint(generation_bp, url_prefix='/api')\n    app.register_blueprint(interaction_bp, url_prefix='/api')\n    app.register_blueprint(admin_bp, url_prefix='/api')\n    \n    return app\n```\n\n### Blueprint Organization\n\n**Analysis Blueprint (`app/routes/analysis.py`):**\n- `POST /api/analyze` - Main contract analysis endpoint\n- `GET /api/analysis/<session_id>` - Retrieve analysis results\n\n**Generation Blueprint (`app/routes/generation.py`):**\n- `POST /api/generate_from_brief` - Generate contract from brief\n- `POST /api/generate_modified_contract` - Create modified compliant versions\n\n**Interaction Blueprint (`app/routes/interaction.py`):**\n- `POST /api/interact` - Real-time Q&A consultation\n- `POST /api/review_modification` - Expert review system\n\n**Admin Blueprint (`app/routes/admin.py`):**\n- `GET /api/rules` - Sharia compliance rules endpoint\n- `GET /api/health` - Application health check\n\n### Enhanced Security Configuration\n\n**Environment-Based Configuration (`config/default.py`):**\n```python\nclass DefaultConfig:\n    # Flask Configuration - REQUIRES environment variable\n    SECRET_KEY = os.environ.get('FLASK_SECRET_KEY')\n    \n    @classmethod\n    def validate_config(cls):\n        if not cls.SECRET_KEY:\n            raise ValueError(\"FLASK_SECRET_KEY environment variable is required\")\n    \n    # External service configuration from environment\n    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n    MONGO_URI = os.environ.get(\"MONGO_URI\")\n    CLOUDINARY_CLOUD_NAME = os.environ.get(\"CLOUDINARY_CLOUD_NAME\")\n    CLOUDINARY_API_KEY = os.environ.get(\"CLOUDINARY_API_KEY\")\n    CLOUDINARY_API_SECRET = os.environ.get(\"CLOUDINARY_API_SECRET\")\n```\n\n**CORS Security:**\n- Restricted origins for development security\n- Credentials support disabled to prevent leakage\n- Configurable for different environments\n\n### Robust Service Initialization\n\n**Database Service (`app/services/database.py`):**\n```python\nclass DatabaseService:\n    def __init__(self, mongo_uri=None):\n        if not mongo_uri:\n            logging.warning(\"MongoDB URI not provided. Database operations will be limited.\")\n            self.client = None\n            return\n        \n        try:\n            self.client = MongoClient(mongo_uri)\n            self.client.admin.command('ping')  # Test connection\n        except Exception as e:\n            logging.error(f\"Failed to connect to MongoDB: {e}\")\n            self.client = None\n```\n\n**Cloudinary Service (`app/services/cloudinary_service.py`):**\n```python\nclass CloudinaryService:\n    def __init__(self, cloud_name=None, api_key=None, api_secret=None):\n        if not all([cloud_name, api_key, api_secret]):\n            logging.warning(\"Cloudinary credentials not fully provided.\")\n            self.is_configured = False\n            return\n        \n        try:\n            cloudinary.config(cloud_name=cloud_name, api_key=api_key, api_secret=api_secret)\n            self.is_configured = True\n        except Exception as e:\n            logging.error(f\"Failed to configure Cloudinary: {e}\")\n            self.is_configured = False\n```\n\n### Prompt Template System\n\nOrganized prompt templates in `prompts/` directory:\n- `sharia_analysis.txt` - AAOIFI compliance analysis\n- `legal_analysis.txt` - General legal review\n- `contract_generation.txt` - Contract creation prompts\n- `interaction_consultation.txt` - User Q&A prompts\n- `modification_review.txt` - Expert review prompts\n\n### Project Structure\n\n```\n├── app/\n│   ├── __init__.py          # Application factory\n│   ├── routes/              # Organized blueprints\n│   │   ├── analysis.py\n│   │   ├── generation.py\n│   │   ├── interaction.py\n│   │   └── admin.py\n│   └── services/            # External service integrations\n│       ├── database.py\n│       └── cloudinary_service.py\n├── config/\n│   ├── default.py          # Environment-based configuration\n│   └── production.py       # Production settings\n├── prompts/                # AI prompt templates\n├── run.py                  # Application entry point\n└── requirements.txt        # Dependencies\n```\n\n## Conclusion\r\n\r\nThis Shariaa Contract Analyzer backend represents a sophisticated integration of modern web technologies, AI capabilities, and document processing systems. The architecture prioritizes scalability, maintainability, and security while providing comprehensive functionality for Islamic law compliance analysis.\r\n\r\nThe system's modular design allows for easy extension and modification, while the comprehensive logging and error handling ensure reliable operation in production environments. The integration with cloud services provides scalability and reliability, making it suitable for enterprise-level deployments.\r\n\r\nRegular monitoring and maintenance procedures ensure optimal performance and reliability, while the comprehensive API design supports both web and mobile client applications.\r\n","size_bytes":29391},"README.md":{"content":"# BackEnd","size_bytes":9},"TECHNICAL_DIAGRAMS.md":{"content":"\r\n# Technical Diagrams and System Architecture\r\n\r\n## System Architecture Diagrams\r\n\r\n### 1. Complete System Architecture\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"Client Tier\"\r\n        WEB[Web Application<br/>React/Vue Frontend]\r\n        MOBILE[Mobile Application<br/>React Native/Flutter]\r\n        API_CLIENT[API Clients<br/>Third-party Integrations]\r\n    end\r\n    \r\n    subgraph \"API Gateway & Load Balancer\"\r\n        LB[Load Balancer<br/>Nginx/HAProxy]\r\n        RATE[Rate Limiter<br/>Redis-based]\r\n    end\r\n    \r\n    subgraph \"Application Tier\"\r\n        direction TB\r\n        FLASK[Flask Application Server<br/>Port 5000<br/>Gunicorn Workers]\r\n        \r\n        subgraph \"Core Services\"\r\n            AUTH[Authentication Service<br/>Session Management]\r\n            ANALYZER[Contract Analysis Engine]\r\n            PROCESSOR[Document Processor]\r\n            GENERATOR[Contract Generator]\r\n            VALIDATOR[Input Validator]\r\n        end\r\n        \r\n        subgraph \"Business Logic\"\r\n            SHARIA[Sharia Compliance Logic]\r\n            TERM[Term Extraction Logic]\r\n            REVIEW[Expert Review Logic]\r\n            MODIFICATION[Modification Engine]\r\n        end\r\n    end\r\n    \r\n    subgraph \"AI/ML Tier\"\r\n        GEMINI[Google Gemini AI<br/>gemini-2.0-flash-thinking]\r\n        EXTRACTION[Document Text Extraction<br/>Vision API]\r\n        NLP[Natural Language Processing<br/>Language Detection]\r\n    end\r\n    \r\n    subgraph \"Data Tier\"\r\n        direction LR\r\n        MONGO[(MongoDB Atlas<br/>Primary Database)]\r\n        REDIS[(Redis Cache<br/>Session Storage)]\r\n        \r\n        subgraph \"Collections\"\r\n            CONTRACTS[contracts collection]\r\n            TERMS[terms collection]\r\n            FEEDBACK[expert_feedback collection]\r\n        end\r\n    end\r\n    \r\n    subgraph \"Storage Tier\"\r\n        CLOUDINARY[Cloudinary CDN<br/>File Storage & Processing]\r\n        TEMP[Local Temporary Storage<br/>Processing Files]\r\n        \r\n        subgraph \"File Types\"\r\n            ORIGINAL[Original Contracts]\r\n            MODIFIED[Modified Contracts]\r\n            MARKED[Marked Contracts]\r\n            PREVIEWS[PDF Previews]\r\n            ANALYSIS[Analysis Results]\r\n        end\r\n    end\r\n    \r\n    subgraph \"External Services\"\r\n        LIBRE[LibreOffice<br/>PDF Conversion]\r\n        SMTP[Email Service<br/>Notifications]\r\n        MONITORING[Monitoring Services<br/>Logs & Metrics]\r\n    end\r\n    \r\n    %% Client connections\r\n    WEB --> LB\r\n    MOBILE --> LB\r\n    API_CLIENT --> LB\r\n    \r\n    %% Load balancer to application\r\n    LB --> RATE\r\n    RATE --> FLASK\r\n    \r\n    %% Application internal connections\r\n    FLASK --> AUTH\r\n    FLASK --> ANALYZER\r\n    FLASK --> PROCESSOR\r\n    FLASK --> GENERATOR\r\n    FLASK --> VALIDATOR\r\n    \r\n    %% Business logic connections\r\n    ANALYZER --> SHARIA\r\n    ANALYZER --> TERM\r\n    PROCESSOR --> MODIFICATION\r\n    GENERATOR --> REVIEW\r\n    \r\n    %% AI service connections\r\n    ANALYZER --> GEMINI\r\n    PROCESSOR --> EXTRACTION\r\n    VALIDATOR --> NLP\r\n    \r\n    %% Database connections\r\n    FLASK --> MONGO\r\n    FLASK --> REDIS\r\n    MONGO --> CONTRACTS\r\n    MONGO --> TERMS\r\n    MONGO --> FEEDBACK\r\n    \r\n    %% Storage connections\r\n    FLASK --> CLOUDINARY\r\n    PROCESSOR --> TEMP\r\n    CLOUDINARY --> ORIGINAL\r\n    CLOUDINARY --> MODIFIED\r\n    CLOUDINARY --> MARKED\r\n    CLOUDINARY --> PREVIEWS\r\n    CLOUDINARY --> ANALYSIS\r\n    \r\n    %% External service connections\r\n    GENERATOR --> LIBRE\r\n    FLASK --> SMTP\r\n    FLASK --> MONITORING\r\n    \r\n    %% Styling\r\n    classDef clientTier fill:#e1f5fe\r\n    classDef appTier fill:#f3e5f5\r\n    classDef aiTier fill:#e8f5e8\r\n    classDef dataTier fill:#fff3e0\r\n    classDef storageTier fill:#fce4ec\r\n    \r\n    class WEB,MOBILE,API_CLIENT clientTier\r\n    class FLASK,AUTH,ANALYZER,PROCESSOR,GENERATOR appTier\r\n    class GEMINI,EXTRACTION,NLP aiTier\r\n    class MONGO,REDIS,CONTRACTS,TERMS,FEEDBACK dataTier\r\n    class CLOUDINARY,TEMP,ORIGINAL,MODIFIED,MARKED,PREVIEWS,ANALYSIS storageTier\r\n```\r\n\r\n### 2. Data Flow Architecture\r\n\r\n```mermaid\r\nsequenceDiagram\r\n    participant C as Client\r\n    participant F as Flask Server\r\n    participant V as Validator\r\n    participant P as Processor\r\n    participant AI as Gemini AI\r\n    participant DB as MongoDB\r\n    participant CL as Cloudinary\r\n    participant G as Generator\r\n    \r\n    Note over C,G: Contract Analysis Flow\r\n    \r\n    C->>+F: POST /analyze (file upload)\r\n    F->>+V: Validate file type & size\r\n    V-->>-F: Validation result\r\n    \r\n    F->>+CL: Upload original file\r\n    CL-->>-F: File URL & metadata\r\n    \r\n    F->>+P: Process document\r\n    P->>P: Extract text & structure\r\n    P->>+AI: Send for analysis\r\n    AI-->>-P: Analysis results (JSON)\r\n    P-->>-F: Structured analysis\r\n    \r\n    F->>+DB: Store contract & terms\r\n    DB-->>-F: Storage confirmation\r\n    \r\n    F->>+CL: Store analysis results\r\n    CL-->>-F: Results URL\r\n    \r\n    F-->>-C: Analysis response + session_id\r\n    \r\n    Note over C,G: Modification Flow\r\n    \r\n    C->>+F: POST /generate_modified_contract\r\n    F->>+DB: Get confirmed modifications\r\n    DB-->>-F: Modification data\r\n    \r\n    F->>+G: Generate modified contract\r\n    G->>G: Apply modifications\r\n    G->>G: Create DOCX & TXT\r\n    G->>+CL: Upload generated files\r\n    CL-->>-G: File URLs\r\n    G-->>-F: Generation results\r\n    \r\n    F->>+DB: Update contract info\r\n    DB-->>-F: Update confirmation\r\n    \r\n    F-->>-C: Generated file URLs\r\n    \r\n    Note over C,G: PDF Preview Flow\r\n    \r\n    C->>+F: GET /preview_contract/{session}/{type}\r\n    F->>+DB: Check existing PDF\r\n    DB-->>-F: PDF info (if exists)\r\n    \r\n    alt PDF exists\r\n        F-->>C: Existing PDF URL\r\n    else Generate new PDF\r\n        F->>+CL: Download source DOCX\r\n        CL-->>-F: DOCX file\r\n        \r\n        F->>F: Convert to PDF (LibreOffice)\r\n        F->>+CL: Upload PDF\r\n        CL-->>-F: PDF URL\r\n        \r\n        F->>+DB: Store PDF info\r\n        DB-->>-F: Storage confirmation\r\n        \r\n        F-->>-C: New PDF URL\r\n    end\r\n```\r\n\r\n### 3. Document Processing Pipeline\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph \"Input Stage\"\r\n        UPLOAD[File Upload<br/>DOCX/PDF/TXT]\r\n        VALIDATE[File Validation<br/>Type, Size, Format]\r\n        STORE_ORIG[Store Original<br/>Cloudinary]\r\n    end\r\n    \r\n    subgraph \"Processing Stage\"\r\n        DETECT{File Type<br/>Detection}\r\n        \r\n        subgraph \"DOCX Processing\"\r\n            DOCX_EXTRACT[python-docx<br/>Text Extraction]\r\n            DOCX_STRUCTURE[Structure Analysis<br/>Paragraphs & Tables]\r\n            DOCX_IDS[Assign Unique IDs<br/>para_X, table_Y_rA_cB]\r\n            DOCX_MARKDOWN[Generate Markdown<br/>with Formatting]\r\n        end\r\n        \r\n        subgraph \"PDF Processing\"\r\n            PDF_AI[AI Text Extraction<br/>Gemini Vision API]\r\n            PDF_CLEAN[Clean Extracted Text<br/>Remove Artifacts]\r\n            PDF_STRUCTURE[Structure Recognition<br/>Headings & Lists]\r\n        end\r\n        \r\n        subgraph \"TXT Processing\"\r\n            TXT_READ[Direct Text Reading<br/>UTF-8 Encoding]\r\n            TXT_STRUCTURE[Basic Structure<br/>Line-by-line]\r\n        end\r\n    end\r\n    \r\n    subgraph \"Analysis Stage\"\r\n        LANG_DETECT[Language Detection<br/>Arabic/English]\r\n        AI_ANALYSIS[AI Analysis<br/>Sharia Compliance]\r\n        JSON_PARSE[Parse AI Response<br/>Extract JSON]\r\n        TERM_EXTRACT[Term Extraction<br/>Individual Clauses]\r\n    end\r\n    \r\n    subgraph \"Storage Stage\"\r\n        DB_STORE[Database Storage<br/>MongoDB]\r\n        CLOUD_STORE[Cloud Storage<br/>Analysis Results]\r\n        SESSION_CREATE[Session Creation<br/>Unique ID]\r\n    end\r\n    \r\n    UPLOAD --> VALIDATE\r\n    VALIDATE --> STORE_ORIG\r\n    STORE_ORIG --> DETECT\r\n    \r\n    DETECT -->|DOCX| DOCX_EXTRACT\r\n    DETECT -->|PDF| PDF_AI\r\n    DETECT -->|TXT| TXT_READ\r\n    \r\n    DOCX_EXTRACT --> DOCX_STRUCTURE\r\n    DOCX_STRUCTURE --> DOCX_IDS\r\n    DOCX_IDS --> DOCX_MARKDOWN\r\n    \r\n    PDF_AI --> PDF_CLEAN\r\n    PDF_CLEAN --> PDF_STRUCTURE\r\n    \r\n    TXT_READ --> TXT_STRUCTURE\r\n    \r\n    DOCX_MARKDOWN --> LANG_DETECT\r\n    PDF_STRUCTURE --> LANG_DETECT\r\n    TXT_STRUCTURE --> LANG_DETECT\r\n    \r\n    LANG_DETECT --> AI_ANALYSIS\r\n    AI_ANALYSIS --> JSON_PARSE\r\n    JSON_PARSE --> TERM_EXTRACT\r\n    \r\n    TERM_EXTRACT --> DB_STORE\r\n    TERM_EXTRACT --> CLOUD_STORE\r\n    TERM_EXTRACT --> SESSION_CREATE\r\n    \r\n    classDef inputStage fill:#e3f2fd\r\n    classDef processStage fill:#f1f8e9\r\n    classDef analysisStage fill:#fff8e1\r\n    classDef storageStage fill:#fce4ec\r\n    \r\n    class UPLOAD,VALIDATE,STORE_ORIG inputStage\r\n    class DETECT,DOCX_EXTRACT,DOCX_STRUCTURE,DOCX_IDS,DOCX_MARKDOWN,PDF_AI,PDF_CLEAN,PDF_STRUCTURE,TXT_READ,TXT_STRUCTURE processStage\r\n    class LANG_DETECT,AI_ANALYSIS,JSON_PARSE,TERM_EXTRACT analysisStage\r\n    class DB_STORE,CLOUD_STORE,SESSION_CREATE storageStage\r\n```\r\n\r\n### 4. AI Integration Architecture\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"AI Service Layer\"\r\n        direction TB\r\n        \r\n        subgraph \"Google Generative AI\"\r\n            GEMINI[Gemini 2.0 Flash<br/>Thinking Model]\r\n            CONFIG[Model Configuration<br/>Temperature: 0<br/>Safety Settings]\r\n            SESSIONS[Chat Sessions<br/>Context Management]\r\n        end\r\n        \r\n        subgraph \"Prompt Engineering\"\r\n            SYS_PROMPT[System Prompts<br/>AAOIFI Standards]\r\n            EXTRACTION[Text Extraction<br/>Prompts]\r\n            INTERACTION[User Interaction<br/>Prompts]\r\n            REVIEW[Modification Review<br/>Prompts]\r\n        end\r\n    end\r\n    \r\n    subgraph \"Processing Engine\"\r\n        direction TB\r\n        \r\n        subgraph \"Input Processing\"\r\n            TEXT_CLEAN[Text Cleaning<br/>& Preprocessing]\r\n            LANG_FORMAT[Language Formatting<br/>Arabic/English]\r\n            STRUCTURE[Structure Preservation<br/>Markdown/IDs]\r\n        end\r\n        \r\n        subgraph \"Response Processing\"\r\n            JSON_EXTRACT[JSON Extraction<br/>from AI Response]\r\n            VALIDATE_RESP[Response Validation<br/>Schema Checking]\r\n            ERROR_HANDLE[Error Handling<br/>Retry Logic]\r\n        end\r\n    end\r\n    \r\n    subgraph \"Application Integration\"\r\n        direction TB\r\n        \r\n        ANALYZER[Contract Analyzer<br/>Main Analysis Logic]\r\n        INTERACTIVE[Interactive Consultation<br/>Q&A System]\r\n        REVIEWER[Modification Reviewer<br/>Expert Validation]\r\n        EXTRACTOR[Document Extractor<br/>PDF/TXT Processing]\r\n    end\r\n    \r\n    %% Connections\r\n    ANALYZER --> TEXT_CLEAN\r\n    INTERACTIVE --> LANG_FORMAT\r\n    REVIEWER --> STRUCTURE\r\n    EXTRACTOR --> TEXT_CLEAN\r\n    \r\n    TEXT_CLEAN --> SYS_PROMPT\r\n    LANG_FORMAT --> INTERACTION\r\n    STRUCTURE --> REVIEW\r\n    \r\n    SYS_PROMPT --> CONFIG\r\n    EXTRACTION --> CONFIG\r\n    INTERACTION --> CONFIG\r\n    REVIEW --> CONFIG\r\n    \r\n    CONFIG --> GEMINI\r\n    GEMINI --> SESSIONS\r\n    \r\n    SESSIONS --> JSON_EXTRACT\r\n    JSON_EXTRACT --> VALIDATE_RESP\r\n    VALIDATE_RESP --> ERROR_HANDLE\r\n    \r\n    ERROR_HANDLE --> ANALYZER\r\n    ERROR_HANDLE --> INTERACTIVE\r\n    ERROR_HANDLE --> REVIEWER\r\n    ERROR_HANDLE --> EXTRACTOR\r\n    \r\n    classDef aiLayer fill:#e8f5e8\r\n    classDef processEngine fill:#fff3e0\r\n    classDef appIntegration fill:#f3e5f5\r\n    \r\n    class GEMINI,CONFIG,SESSIONS,SYS_PROMPT,EXTRACTION,INTERACTION,REVIEW aiLayer\r\n    class TEXT_CLEAN,LANG_FORMAT,STRUCTURE,JSON_EXTRACT,VALIDATE_RESP,ERROR_HANDLE processEngine\r\n    class ANALYZER,INTERACTIVE,REVIEWER,EXTRACTOR appIntegration\r\n```\r\n\r\n### 5. Database Schema Relationships\r\n\r\n```mermaid\r\nerDiagram\r\n    CONTRACTS {\r\n        string _id PK \"Session ID\"\r\n        string session_id UK \"Unique Session\"\r\n        string original_filename\r\n        object original_cloudinary_info\r\n        object analysis_results_cloudinary_info\r\n        string original_format \"docx|pdf|txt\"\r\n        text original_contract_plain\r\n        text original_contract_markdown\r\n        text generated_markdown_from_docx\r\n        string detected_contract_language \"ar|en\"\r\n        datetime analysis_timestamp\r\n        object confirmed_terms \"term_id -> modification\"\r\n        array interactions \"User Q&A history\"\r\n        object modified_contract_info\r\n        object marked_contract_info\r\n        object pdf_preview_info\r\n    }\r\n    \r\n    TERMS {\r\n        objectid _id PK\r\n        string session_id FK\r\n        string term_id UK \"Unique per session\"\r\n        text term_text\r\n        boolean is_valid_sharia\r\n        text sharia_issue\r\n        text reference_number\r\n        text modified_term\r\n        boolean is_confirmed_by_user\r\n        text confirmed_modified_text\r\n        boolean has_expert_feedback\r\n        objectid last_expert_feedback_id FK\r\n        boolean expert_override_is_valid_sharia\r\n    }\r\n    \r\n    EXPERT_FEEDBACK {\r\n        objectid _id PK\r\n        string session_id FK\r\n        string term_id FK\r\n        text original_term_text_snapshot\r\n        string expert_user_id\r\n        string expert_username\r\n        datetime feedback_timestamp\r\n        object ai_initial_analysis_assessment\r\n        boolean expert_verdict_is_valid_sharia\r\n        text expert_comment_on_term\r\n        text expert_corrected_sharia_issue\r\n        text expert_corrected_reference\r\n        text expert_final_suggestion_for_term\r\n        boolean original_ai_is_valid_sharia \"Snapshot\"\r\n        text original_ai_sharia_issue \"Snapshot\"\r\n        text original_ai_modified_term \"Snapshot\"\r\n        text original_ai_reference_number \"Snapshot\"\r\n    }\r\n    \r\n    SESSIONS {\r\n        string session_id PK \"Redis Key\"\r\n        datetime created_at\r\n        datetime last_accessed\r\n        object user_preferences\r\n        boolean is_active\r\n    }\r\n    \r\n    %% Relationships\r\n    CONTRACTS ||--o{ TERMS : \"has many terms\"\r\n    TERMS ||--o{ EXPERT_FEEDBACK : \"can have feedback\"\r\n    CONTRACTS ||--o| SESSIONS : \"linked to session\"\r\n    \r\n    %% Indexes\r\n    CONTRACTS {\r\n        index session_id_idx \"session_id\"\r\n        index timestamp_idx \"analysis_timestamp\"\r\n        index language_idx \"detected_contract_language\"\r\n    }\r\n    \r\n    TERMS {\r\n        compound_index session_term_idx \"session_id, term_id\"\r\n        index valid_sharia_idx \"is_valid_sharia\"\r\n        index confirmed_idx \"is_confirmed_by_user\"\r\n    }\r\n    \r\n    EXPERT_FEEDBACK {\r\n        compound_index session_term_feedback_idx \"session_id, term_id\"\r\n        index expert_idx \"expert_user_id\"\r\n        index timestamp_idx \"feedback_timestamp\"\r\n    }\r\n```\r\n\r\n### 6. Security Architecture\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"External Threats\"\r\n        DDOS[DDoS Attacks]\r\n        INJECTION[Injection Attacks]\r\n        XSS[Cross-Site Scripting]\r\n        CSRF[CSRF Attacks]\r\n        FILE_UPLOAD[Malicious File Uploads]\r\n    end\r\n    \r\n    subgraph \"Defense Layer 1: Network Security\"\r\n        CDN[Cloudflare CDN<br/>DDoS Protection]\r\n        FIREWALL[Web Application Firewall<br/>SQL Injection Prevention]\r\n        RATE_LIMIT[Rate Limiting<br/>API Throttling]\r\n    end\r\n    \r\n    subgraph \"Defense Layer 2: Application Security\"\r\n        INPUT_VAL[Input Validation<br/>File Type & Size Checks]\r\n        SANITIZE[Data Sanitization<br/>XSS Prevention]\r\n        CORS[CORS Configuration<br/>Origin Restrictions]\r\n        HEADERS[Security Headers<br/>CSP, HSTS, X-Frame-Options]\r\n    end\r\n    \r\n    subgraph \"Defense Layer 3: Data Security\"\r\n        ENCRYPT_TRANSIT[Encryption in Transit<br/>HTTPS/TLS 1.3]\r\n        ENCRYPT_REST[Encryption at Rest<br/>MongoDB Encryption]\r\n        SESSION_SEC[Secure Sessions<br/>HTTPOnly, Secure Cookies]\r\n        API_KEY[API Key Management<br/>Environment Variables]\r\n    end\r\n    \r\n    subgraph \"Defense Layer 4: Infrastructure Security\"\r\n        ACCESS_CONTROL[Access Control<br/>IAM Policies]\r\n        AUDIT_LOG[Audit Logging<br/>All Actions Tracked]\r\n        BACKUP_SEC[Secure Backups<br/>Encrypted Storage]\r\n        MONITORING[Security Monitoring<br/>Intrusion Detection]\r\n    end\r\n    \r\n    subgraph \"Internal Systems\"\r\n        FLASK_APP[Flask Application]\r\n        DATABASE[MongoDB Atlas]\r\n        CLOUD_STORAGE[Cloudinary]\r\n        AI_SERVICE[Google AI]\r\n    end\r\n    \r\n    %% Threat flows\r\n    DDOS --> CDN\r\n    INJECTION --> FIREWALL\r\n    XSS --> SANITIZE\r\n    CSRF --> HEADERS\r\n    FILE_UPLOAD --> INPUT_VAL\r\n    \r\n    %% Defense flows\r\n    CDN --> RATE_LIMIT\r\n    FIREWALL --> INPUT_VAL\r\n    RATE_LIMIT --> CORS\r\n    \r\n    INPUT_VAL --> ENCRYPT_TRANSIT\r\n    SANITIZE --> SESSION_SEC\r\n    CORS --> API_KEY\r\n    HEADERS --> ENCRYPT_REST\r\n    \r\n    ENCRYPT_TRANSIT --> ACCESS_CONTROL\r\n    ENCRYPT_REST --> AUDIT_LOG\r\n    SESSION_SEC --> BACKUP_SEC\r\n    API_KEY --> MONITORING\r\n    \r\n    ACCESS_CONTROL --> FLASK_APP\r\n    AUDIT_LOG --> DATABASE\r\n    BACKUP_SEC --> CLOUD_STORAGE\r\n    MONITORING --> AI_SERVICE\r\n    \r\n    classDef threat fill:#ffcdd2\r\n    classDef defense1 fill:#c8e6c9\r\n    classDef defense2 fill:#dcedc8\r\n    classDef defense3 fill:#f0f4c3\r\n    classDef defense4 fill:#fff9c4\r\n    classDef internal fill:#e1f5fe\r\n    \r\n    class DDOS,INJECTION,XSS,CSRF,FILE_UPLOAD threat\r\n    class CDN,FIREWALL,RATE_LIMIT defense1\r\n    class INPUT_VAL,SANITIZE,CORS,HEADERS defense2\r\n    class ENCRYPT_TRANSIT,ENCRYPT_REST,SESSION_SEC,API_KEY defense3\r\n    class ACCESS_CONTROL,AUDIT_LOG,BACKUP_SEC,MONITORING defense4\r\n    class FLASK_APP,DATABASE,CLOUD_STORAGE,AI_SERVICE internal\r\n```\r\n\r\n### 7. Performance Monitoring Dashboard\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"Performance Metrics Dashboard\"\r\n        \r\n        subgraph \"Response Time Metrics\"\r\n            RT_API[API Response Times<br/>P50, P95, P99]\r\n            RT_AI[AI Processing Times<br/>Analysis Duration]\r\n            RT_DB[Database Query Times<br/>Read/Write Performance]\r\n            RT_STORAGE[Storage Operations<br/>Upload/Download Times]\r\n        end\r\n        \r\n        subgraph \"Throughput Metrics\"\r\n            TH_REQUESTS[Requests per Second<br/>Peak & Average]\r\n            TH_ANALYSIS[Contracts Analyzed<br/>per Hour]\r\n            TH_GENERATION[Documents Generated<br/>per Hour]\r\n            TH_INTERACTIONS[User Interactions<br/>per Minute]\r\n        end\r\n        \r\n        subgraph \"Error Rate Metrics\"\r\n            ER_HTTP[HTTP Error Rates<br/>4xx, 5xx Responses]\r\n            ER_AI[AI Service Failures<br/>Timeout & API Errors]\r\n            ER_DB[Database Errors<br/>Connection & Query Failures]\r\n            ER_STORAGE[Storage Failures<br/>Upload & Download Errors]\r\n        end\r\n        \r\n        subgraph \"Resource Utilization\"\r\n            RU_CPU[CPU Utilization<br/>Application Server]\r\n            RU_MEMORY[Memory Usage<br/>Heap & Process Memory]\r\n            RU_DISK[Disk Usage<br/>Temporary Files]\r\n            RU_NETWORK[Network Bandwidth<br/>Ingress & Egress]\r\n        end\r\n        \r\n        subgraph \"Business Metrics\"\r\n            BM_COMPLIANCE[Compliance Rate<br/>Valid vs Invalid Terms]\r\n            BM_SATISFACTION[User Satisfaction<br/>Completion Rate]\r\n            BM_EXPERTISE[Expert Reviews<br/>Override Rate]\r\n            BM_CONVERSION[Contract Generation<br/>Success Rate]\r\n        end\r\n        \r\n        subgraph \"Alerting System\"\r\n            ALERT_PERF[Performance Alerts<br/>Response Time SLA]\r\n            ALERT_ERROR[Error Rate Alerts<br/>Threshold Breaches]\r\n            ALERT_RESOURCE[Resource Alerts<br/>CPU/Memory Limits]\r\n            ALERT_BUSINESS[Business Alerts<br/>Compliance Issues]\r\n        end\r\n    end\r\n    \r\n    %% Metric relationships\r\n    RT_API --> ALERT_PERF\r\n    RT_AI --> ALERT_PERF\r\n    RT_DB --> ALERT_PERF\r\n    RT_STORAGE --> ALERT_PERF\r\n    \r\n    ER_HTTP --> ALERT_ERROR\r\n    ER_AI --> ALERT_ERROR\r\n    ER_DB --> ALERT_ERROR\r\n    ER_STORAGE --> ALERT_ERROR\r\n    \r\n    RU_CPU --> ALERT_RESOURCE\r\n    RU_MEMORY --> ALERT_RESOURCE\r\n    RU_DISK --> ALERT_RESOURCE\r\n    RU_NETWORK --> ALERT_RESOURCE\r\n    \r\n    BM_COMPLIANCE --> ALERT_BUSINESS\r\n    BM_SATISFACTION --> ALERT_BUSINESS\r\n    BM_EXPERTISE --> ALERT_BUSINESS\r\n    BM_CONVERSION --> ALERT_BUSINESS\r\n    \r\n    classDef metrics fill:#e3f2fd\r\n    classDef alerts fill:#ffebee\r\n    \r\n    class RT_API,RT_AI,RT_DB,RT_STORAGE,TH_REQUESTS,TH_ANALYSIS,TH_GENERATION,TH_INTERACTIONS,ER_HTTP,ER_AI,ER_DB,ER_STORAGE,RU_CPU,RU_MEMORY,RU_DISK,RU_NETWORK,BM_COMPLIANCE,BM_SATISFACTION,BM_EXPERTISE,BM_CONVERSION metrics\r\n    class ALERT_PERF,ALERT_ERROR,ALERT_RESOURCE,ALERT_BUSINESS alerts\r\n```\r\n\r\n### 8. Deployment Pipeline\r\n\r\n```mermaid\r\ngraph LR\r\n    subgraph \"Development Environment\"\r\n        DEV_CODE[Local Development<br/>Python 3.12]\r\n        DEV_TEST[Unit Testing<br/>pytest]\r\n        DEV_LINT[Code Linting<br/>flake8, black]\r\n    end\r\n    \r\n    subgraph \"Version Control\"\r\n        GIT[Git Repository<br/>Version Control]\r\n        PR[Pull Request<br/>Code Review]\r\n        MERGE[Merge to Main<br/>Automated Checks]\r\n    end\r\n    \r\n    subgraph \"CI/CD Pipeline\"\r\n        BUILD[Build Process<br/>Dependencies Install]\r\n        TEST_INTEGRATION[Integration Tests<br/>API Testing]\r\n        SECURITY_SCAN[Security Scanning<br/>Vulnerability Check]\r\n        QUALITY_GATE[Quality Gate<br/>Coverage & Standards]\r\n    end\r\n    \r\n    subgraph \"Staging Environment\"\r\n        STAGING_DEPLOY[Staging Deployment<br/>Replit Staging]\r\n        STAGING_TEST[End-to-End Testing<br/>User Scenarios]\r\n        PERFORMANCE_TEST[Performance Testing<br/>Load & Stress]\r\n    end\r\n    \r\n    subgraph \"Production Environment\"\r\n        PROD_DEPLOY[Production Deployment<br/>Replit Production]\r\n        HEALTH_CHECK[Health Checks<br/>System Validation]\r\n        MONITORING_SETUP[Monitoring Setup<br/>Alerts & Logging]\r\n        ROLLBACK[Rollback Strategy<br/>Quick Recovery]\r\n    end\r\n    \r\n    subgraph \"Post-Deployment\"\r\n        SMOKE_TEST[Smoke Testing<br/>Critical Path Validation]\r\n        METRICS[Metrics Collection<br/>Performance Monitoring]\r\n        USER_FEEDBACK[User Feedback<br/>System Performance]\r\n    end\r\n    \r\n    %% Flow connections\r\n    DEV_CODE --> DEV_TEST\r\n    DEV_TEST --> DEV_LINT\r\n    DEV_LINT --> GIT\r\n    \r\n    GIT --> PR\r\n    PR --> MERGE\r\n    MERGE --> BUILD\r\n    \r\n    BUILD --> TEST_INTEGRATION\r\n    TEST_INTEGRATION --> SECURITY_SCAN\r\n    SECURITY_SCAN --> QUALITY_GATE\r\n    \r\n    QUALITY_GATE --> STAGING_DEPLOY\r\n    STAGING_DEPLOY --> STAGING_TEST\r\n    STAGING_TEST --> PERFORMANCE_TEST\r\n    \r\n    PERFORMANCE_TEST --> PROD_DEPLOY\r\n    PROD_DEPLOY --> HEALTH_CHECK\r\n    HEALTH_CHECK --> MONITORING_SETUP\r\n    MONITORING_SETUP --> ROLLBACK\r\n    \r\n    ROLLBACK --> SMOKE_TEST\r\n    SMOKE_TEST --> METRICS\r\n    METRICS --> USER_FEEDBACK\r\n    \r\n    classDef development fill:#e8f5e8\r\n    classDef versionControl fill:#fff3e0\r\n    classDef cicd fill:#f3e5f5\r\n    classDef staging fill:#e1f5fe\r\n    classDef production fill:#ffebee\r\n    classDef postDeploy fill:#f9fbe7\r\n    \r\n    class DEV_CODE,DEV_TEST,DEV_LINT development\r\n    class GIT,PR,MERGE versionControl\r\n    class BUILD,TEST_INTEGRATION,SECURITY_SCAN,QUALITY_GATE cicd\r\n    class STAGING_DEPLOY,STAGING_TEST,PERFORMANCE_TEST staging\r\n    class PROD_DEPLOY,HEALTH_CHECK,MONITORING_SETUP,ROLLBACK production\r\n    class SMOKE_TEST,METRICS,USER_FEEDBACK postDeploy\r\n```\r\n\r\n## Performance Benchmark Charts\r\n\r\n### API Response Time Distribution\r\n\r\n```\r\nAPI Endpoint Performance (ms)\r\n╭─────────────────────────────────────────────────────────╮\r\n│                                                         │\r\n│  /analyze           ████████████████████▓▓ 2800ms (P95) │\r\n│                     ████████████▓▓ 1800ms (P50)         │\r\n│                                                         │\r\n│  /generate_modified ████████████▓▓ 1200ms (P95)         │\r\n│                     ████▓▓ 600ms (P50)                  │\r\n│                                                         │\r\n│  /interact          ███▓▓ 450ms (P95)                   │\r\n│                     ▓▓ 200ms (P50)                      │\r\n│                                                         │\r\n│  /preview_contract  ████████▓▓ 900ms (P95)              │\r\n│                     ███▓▓ 400ms (P50)                   │\r\n│                                                         │\r\n│  /terms             ▓ 80ms (P95)                        │\r\n│                     ▓ 40ms (P50)                        │\r\n│                                                         │\r\n╰─────────────────────────────────────────────────────────╯\r\n```\r\n\r\n### System Resource Utilization\r\n\r\n```\r\nResource Utilization Over Time\r\n╭─────────────────────────────────────────────────────────╮\r\n│ CPU %                                                   │\r\n│ 100├─────────────────────────────────────────────────── │\r\n│  80│        ████                    ████                │\r\n│  60│    ████    ████            ████    ████            │\r\n│  40│████            ████    ████            ████        │\r\n│  20│                    ████                    ████    │\r\n│   0└─────────────────────────────────────────────────── │\r\n│                                                         │\r\n│ Memory (GB)                                             │\r\n│   8├─────────────────────────────────────────────────── │\r\n│   6│                    ████████████████████████████    │\r\n│   4│            ████████                                │\r\n│   2│    ████████                                        │\r\n│   0└─────────────────────────────────────────────────── │\r\n│    0    5    10   15   20   25   30   35   40   45   50 │\r\n│                        Time (minutes)                   │\r\n╰─────────────────────────────────────────────────────────╯\r\n```\r\n\r\n### Error Rate Tracking\r\n\r\n```\r\nError Rates by Category (Last 24 Hours)\r\n╭─────────────────────────────────────────────────────────╮\r\n│                                                         │\r\n│ HTTP 4xx Errors      ██▓ 2.3%                          │\r\n│ HTTP 5xx Errors      ▓ 0.8%                            │\r\n│ AI Service Failures  █▓ 1.5%                           │\r\n│ Database Timeouts    ▓ 0.3%                            │\r\n│ Storage Failures     ▓ 0.2%                            │\r\n│                                                         │\r\n│ Total Error Rate: 5.1%                                 │\r\n│ SLA Target: <5.0% ❌                                    │\r\n│                                                         │\r\n╰─────────────────────────────────────────────────────────╯\r\n```\r\n\r\nThis comprehensive technical documentation provides deep insights into the Shariaa Contract Analyzer backend architecture, including detailed diagrams, performance metrics, and technical specifications. The documentation covers all aspects from high-level architecture to implementation details, making it suitable for both technical teams and stakeholders.\r\n","size_bytes":28165},"config.py":{"content":"# NOTE: shim — kept for backward compatibility\n# Configuration moved to config/default.py and prompts/ directory\n\nimport os\nfrom config.default import DefaultConfig\n\n# Re-export environment variables for compatibility\nCLOUDINARY_CLOUD_NAME = os.environ.get(\"CLOUDINARY_CLOUD_NAME\")\nCLOUDINARY_API_KEY = os.environ.get(\"CLOUDINARY_API_KEY\")\nCLOUDINARY_API_SECRET = os.environ.get(\"CLOUDINARY_API_SECRET\")\nCLOUDINARY_BASE_FOLDER = \"shariaa_analyzer_uploads\"\n\nCLOUDINARY_UPLOAD_FOLDER = \"contract_uploads\"\nCLOUDINARY_ORIGINAL_UPLOADS_SUBFOLDER = \"original_contracts\"\nCLOUDINARY_ANALYSIS_RESULTS_SUBFOLDER = \"analysis_results_json\"\nCLOUDINARY_MODIFIED_CONTRACTS_SUBFOLDER = \"modified_contracts\"\nCLOUDINARY_MARKED_CONTRACTS_SUBFOLDER = \"marked_contracts\"\nCLOUDINARY_PDF_PREVIEWS_SUBFOLDER = \"pdf_previews\"\n\nGOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\nMODEL_NAME = \"gemini-2.5-flash\"\nTEMPERATURE = 0\nMONGO_URI = os.environ.get(\"MONGO_URI\")\nLIBREOFFICE_PATH = \"libreoffice\"\nFLASK_SECRET_KEY = os.environ.get(\"FLASK_SECRET_KEY\", \"your_secret_key_here\")\n\n# Load prompts from files\ndef load_prompt(filename):\n    \"\"\"Load a prompt from the prompts directory.\"\"\"\n    try:\n        with open(f'prompts/{filename}', 'r', encoding='utf-8') as f:\n            return f.read().strip()\n    except FileNotFoundError:\n        return f\"ERROR: Prompt file {filename} not found\"\n\nEXTRACTION_PROMPT = load_prompt('EXTRACTION_PROMPT.txt')\nSYS_PROMPT = load_prompt('SYS_PROMPT_SHARIA_ANALYSIS.txt')\nINTERACTION_PROMPT = load_prompt('INTERACTION_PROMPT_SHARIA.txt')\nREVIEW_MODIFICATION_PROMPT = load_prompt('REVIEW_MODIFICATION_PROMPT_SHARIA.txt')\nCONTRACT_REGENERATION_PROMPT = load_prompt('CONTRACT_REGENERATION_PROMPT.txt')","size_bytes":1705},"doc_processing.py":{"content":"# NOTE: shim — kept for backward compatibility\n# All functionality moved to app/services/document_processor.py\n\nfrom app.services.document_processor import (\n    build_structured_text_for_analysis,\n    create_docx_from_llm_markdown,\n    convert_docx_to_pdf\n)\n\n__all__ = [\n    'build_structured_text_for_analysis',\n    'create_docx_from_llm_markdown', \n    'convert_docx_to_pdf'\n]","size_bytes":381},"remote_api.py":{"content":"# NOTE: shim — kept for backward compatibility\n# All functionality moved to app/services/ai_service.py\n\nfrom app.services.ai_service import get_chat_session, send_text_to_remote_api, extract_text_from_file\n\n__all__ = ['get_chat_session', 'send_text_to_remote_api', 'extract_text_from_file']","size_bytes":292},"replit.md":{"content":"# Shariaa Contract Analyzer Backend\n\n## Overview\n\nThe Shariaa Contract Analyzer is a sophisticated Flask-based backend system designed to analyze legal contracts for compliance with Islamic law (Sharia) principles, specifically following AAOIFI (Accounting and Auditing Organization for Islamic Financial Institutions) standards. The system also supports general legal compliance analysis for various jurisdictions.\n\n**Key Features:**\n- Multi-format contract processing (DOCX, PDF, TXT)\n- AI-powered compliance analysis using Google Gemini 2.0 Flash\n- Interactive user consultation with real-time Q&A\n- Expert review system integration\n- Automated contract modification and regeneration\n- Cloud-based document management with Cloudinary\n- Multi-language support (Arabic and English)\n- Modular architecture with service-oriented design\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Application Framework\n- **Flask Application Factory Pattern**: Modular Flask setup with blueprints for route organization\n- **Service Layer Architecture**: Clear separation between routes, services, and utilities\n- **Configuration Management**: Environment-based configuration with secure defaults\n\n### Core Services\n- **Database Service**: MongoDB Atlas integration for document storage with graceful fallback handling\n- **AI Service**: Google Gemini 2.0 Flash integration for contract analysis and text processing\n- **Cloud Storage Service**: Cloudinary integration for document management with automatic fallbacks\n- **Document Processing Service**: LibreOffice headless conversion for DOCX to PDF transformation\n\n### API Architecture\n- **RESTful Design**: Well-structured endpoints following REST principles\n- **Blueprint Organization**: Routes separated by functional areas (analysis, generation, interaction, admin)\n- **Comprehensive Error Handling**: Graceful degradation when services are unavailable\n- **CORS Configuration**: Configured for web client integration\n\n### Data Architecture\n- **Document Schema**: Structured storage for contracts, analysis results, and user interactions\n- **Session Management**: UUID-based session tracking for multi-step processes\n- **Term Extraction**: Structured term identification with unique IDs for precise modification tracking\n\n### Processing Pipeline\n- **File Upload Processing**: Multi-format support with secure filename handling\n- **Text Extraction**: AI-powered text extraction preserving document structure\n- **Compliance Analysis**: Dual-mode analysis (Sharia/Legal) with jurisdiction support\n- **Interactive Consultation**: Real-time Q&A with context-aware responses\n- **Contract Modification**: User-guided modification with expert review integration\n\n### Prompt Management System\n- **Structured Prompt Library**: Organized prompt files for different analysis types and languages\n- **Template-based Approach**: Parameterized prompts for consistent AI interactions\n- **Multi-language Support**: Language-specific prompts for Arabic and English analysis\n\n### Security & Performance\n- **Input Validation**: Comprehensive input sanitization and file type checking\n- **Rate Limiting**: Built-in protection against abuse\n- **Secure File Handling**: Temporary file management with automatic cleanup\n- **Resource Management**: Optimized for cloud deployment with configurable workers\n\n## External Dependencies\n\n### AI Services\n- **Google Generative AI (Gemini 2.0 Flash)**: Primary AI engine for contract analysis, text extraction, and natural language processing\n- **Language Detection**: Automatic language detection for appropriate prompt selection\n\n### Database\n- **MongoDB Atlas**: Primary database for storing contracts, analysis results, terms, and expert feedback\n- **Redis Cache**: Session storage and caching layer (referenced in architecture docs)\n\n### Cloud Storage\n- **Cloudinary**: Document storage and management with organized folder structure for different document types\n\n### Document Processing\n- **LibreOffice Headless**: Server-side document conversion (DOCX to PDF)\n- **python-docx**: DOCX document manipulation and generation\n- **unidecode**: Text normalization and filename sanitization\n\n### Web Framework & Infrastructure\n- **Flask**: Core web framework with CORS support\n- **Gunicorn**: WSGI server for production deployment\n- **Werkzeug**: Secure file upload handling\n\n### Development & Monitoring\n- **Logging Infrastructure**: Comprehensive logging system for debugging and monitoring\n- **Replit Environment**: Optimized for Replit hosting with appropriate bindings and configurations\n\n### Configuration Management\n- **Environment Variables**: Secure credential management for API keys and database connections\n- **Multi-environment Support**: Configuration classes for development, testing, and production","size_bytes":4827},"run.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nFlask application entry point for the Shariaa Analyzer backend.\nThis file serves as the main entry point for both development and production.\n\"\"\"\n\nfrom app import create_app\nimport os\n\n# Create Flask app using factory pattern\napp = create_app()\n\nif __name__ == \"__main__\":\n    # Development server configuration\n    port = int(os.environ.get('PORT', 5000))\n    debug = os.environ.get('DEBUG', 'True').lower() == 'true'\n    \n    # Configure for Replit environment - bind to all interfaces\n    app.run(\n        host='0.0.0.0',\n        port=port,\n        debug=debug,\n        use_reloader=False  # Disable reloader to prevent issues in Replit\n    )","size_bytes":672},"utils.py":{"content":"# NOTE: shim — kept for backward compatibility\n# All functionality moved to app/utils/\n\nfrom app.utils.file_helpers import ensure_dir, clean_filename, download_file_from_url\nfrom app.utils.text_processing import clean_model_response\nfrom app.services.cloudinary_service import upload_to_cloudinary_helper\n\n__all__ = [\n    'ensure_dir', 'clean_filename', 'clean_model_response', \n    'download_file_from_url', 'upload_to_cloudinary_helper'\n]","size_bytes":442},"app/__init__.py":{"content":"\"\"\"\nFlask Application Factory\n\nThis module provides the Flask application factory pattern for the Shariaa Contract Analyzer.\n\"\"\"\n\nimport os\nimport logging\nfrom flask import Flask\nfrom flask_cors import CORS\n\n\ndef create_app(config_name='default'):\n    \"\"\"\n    Create and configure Flask application instance.\n    \n    Args:\n        config_name (str): Configuration environment name\n        \n    Returns:\n        Flask: Configured Flask application instance\n    \"\"\"\n    app = Flask(__name__)\n    \n    # Load configuration\n    if config_name == 'testing':\n        app.config.from_object('config.testing.TestingConfig')\n    elif config_name == 'production':\n        app.config.from_object('config.production.ProductionConfig')\n    else:\n        app.config.from_object('config.default.DefaultConfig')\n    \n    # Validate critical configuration\n    if not app.config.get('SECRET_KEY'):\n        error_msg = \"FLASK_SECRET_KEY environment variable is required\"\n        logging.error(error_msg)\n        if config_name == 'production':\n            raise ValueError(error_msg)\n        else:\n            logging.warning(\"Running with insecure default SECRET_KEY for development\")\n    \n    # Configure CORS for Replit environment - allow all origins for development\n    CORS(app, origins=\"*\", supports_credentials=False)\n    \n    # Set maximum content length (16MB)\n    app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n    \n    # Configure logging\n    configure_logging(app)\n    \n    # Initialize services\n    from app.services.database import init_db\n    init_db(app)\n    \n    from app.services.cloudinary_service import init_cloudinary\n    init_cloudinary(app)\n    \n    from app.services.ai_service import init_ai_service\n    init_ai_service(app)\n    \n    # Register blueprints\n    register_blueprints(app)\n    \n    return app\n\n\ndef configure_logging(app):\n    \"\"\"Configure application logging.\"\"\"\n    if not app.debug and not app.testing:\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.StreamHandler(),\n                logging.FileHandler('shariaa_analyzer.log', encoding='utf-8')\n            ]\n        )\n\n\ndef register_blueprints(app):\n    \"\"\"Register application blueprints.\"\"\"\n    from app.routes.analysis import analysis_bp\n    from app.routes.generation import generation_bp\n    from app.routes.interaction import interaction_bp\n    from app.routes.admin import admin_bp\n    \n    app.register_blueprint(analysis_bp, url_prefix='/api')\n    app.register_blueprint(generation_bp, url_prefix='/api')\n    app.register_blueprint(interaction_bp, url_prefix='/api')\n    app.register_blueprint(admin_bp, url_prefix='/api/admin')","size_bytes":2753},"config/__init__.py":{"content":"# Configuration package","size_bytes":23},"config/default.py":{"content":"\"\"\"\nDefault Configuration\n\nConfiguration settings for the Shariaa Contract Analyzer.\n\"\"\"\n\nimport os\n\nclass DefaultConfig:\n    \"\"\"Default configuration settings.\"\"\"\n    \n    # Flask Configuration\n    SECRET_KEY = os.environ.get('FLASK_SECRET_KEY')\n    DEBUG = False  # Secure default\n    \n    @classmethod\n    def validate_config(cls):\n        \"\"\"Validate required configuration values.\"\"\"\n        if not cls.SECRET_KEY:\n            raise ValueError(\"FLASK_SECRET_KEY environment variable is required\")\n    \n    # AI Service Configuration\n    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n    MODEL_NAME = \"gemini-2.5-flash\"\n    TEMPERATURE = 0\n    \n    # Database Configuration\n    MONGO_URI = os.environ.get(\"MONGO_URI\")\n    \n    # Cloud Storage Configuration\n    CLOUDINARY_CLOUD_NAME = os.environ.get(\"CLOUDINARY_CLOUD_NAME\")\n    CLOUDINARY_API_KEY = os.environ.get(\"CLOUDINARY_API_KEY\")\n    CLOUDINARY_API_SECRET = os.environ.get(\"CLOUDINARY_API_SECRET\")\n    CLOUDINARY_BASE_FOLDER = \"shariaa_analyzer_uploads\"\n    \n    # Cloudinary Subfolder Structure\n    CLOUDINARY_UPLOAD_FOLDER = \"contract_uploads\"\n    CLOUDINARY_ORIGINAL_UPLOADS_SUBFOLDER = \"original_contracts\"\n    CLOUDINARY_ANALYSIS_RESULTS_SUBFOLDER = \"analysis_results_json\"\n    CLOUDINARY_MODIFIED_CONTRACTS_SUBFOLDER = \"modified_contracts\"\n    CLOUDINARY_MARKED_CONTRACTS_SUBFOLDER = \"marked_contracts\"\n    CLOUDINARY_PDF_PREVIEWS_SUBFOLDER = \"pdf_previews\"\n    \n    # External Tools\n    LIBREOFFICE_PATH = \"libreoffice\"  # System-wide LibreOffice installation\n    \n    # Default Jurisdiction\n    DEFAULT_JURISDICTION = \"Egypt\"\n    \n    # AI Prompts (read from prompts/ directory)\n    @classmethod\n    def load_prompt(cls, filename):\n        \"\"\"Load a prompt from the prompts directory.\"\"\"\n        try:\n            prompt_path = os.path.join('prompts', filename)\n            with open(prompt_path, 'r', encoding='utf-8') as f:\n                return f.read().strip()\n        except FileNotFoundError:\n            return f\"ERROR: Prompt file {filename} not found\"\n    \n    @property\n    def EXTRACTION_PROMPT(self):\n        \"\"\"Load extraction prompt from file.\"\"\"\n        return self.load_prompt('EXTRACTION_PROMPT.txt')\n    \n    @property\n    def SYS_PROMPT_SHARIA(self):\n        \"\"\"Load Sharia analysis prompt from file.\"\"\"\n        return self.load_prompt('SYS_PROMPT_SHARIA_ANALYSIS.txt')\n    \n    @property\n    def SYS_PROMPT_LEGAL(self):\n        \"\"\"Load Legal analysis prompt from file.\"\"\"\n        return self.load_prompt('SYS_PROMPT_LEGAL_ANALYSIS.txt')\n    \n    @property\n    def INTERACTION_PROMPT_SHARIA(self):\n        \"\"\"Load Sharia interaction prompt from file.\"\"\"\n        return self.load_prompt('INTERACTION_PROMPT_SHARIA.txt')\n    \n    @property\n    def REVIEW_MODIFICATION_PROMPT_SHARIA(self):\n        \"\"\"Load Sharia review modification prompt from file.\"\"\"\n        return self.load_prompt('REVIEW_MODIFICATION_PROMPT_SHARIA.txt')\n    \n    @property\n    def CONTRACT_REGENERATION_PROMPT(self):\n        \"\"\"Load contract regeneration prompt from file.\"\"\"\n        return self.load_prompt('CONTRACT_REGENERATION_PROMPT.txt')\n    \n    @property\n    def CONTRACT_GENERATION_PROMPT(self):\n        \"\"\"Load contract generation prompt from file.\"\"\"\n        return self.load_prompt('CONTRACT_GENERATION_PROMPT.txt')\n    \n    @property\n    def INTERACTION_PROMPT_LEGAL(self):\n        \"\"\"Load Legal interaction prompt from file.\"\"\"\n        return self.load_prompt('INTERACTION_PROMPT_LEGAL.txt')\n    \n    @property\n    def REVIEW_MODIFICATION_PROMPT_LEGAL(self):\n        \"\"\"Load Legal review modification prompt from file.\"\"\"\n        return self.load_prompt('REVIEW_MODIFICATION_PROMPT_LEGAL.txt')","size_bytes":3669},"migrations/api_server_move_report.md":{"content":"# Migration Report: api_server.py\n\n**Original file:** `api_server.py` (1,312 lines)\n**Migration date:** September 14, 2025\n\n## Exported Functions/Classes/Routes\n\n### Main Application\n- **Flask app initialization** -> MOVED to `app/__init__.py:create_app()`\n- **CORS configuration** -> MOVED to `app/__init__.py:create_app()`\n\n### API Routes (need to be moved to appropriate app/routes/ files)\n- `@app.route(\"/analyze\", methods=[\"POST\"])` -> NEEDS MOVE to `app/routes/analysis.py`\n- `@app.route(\"/preview_contract/<session_id>/<contract_type>\", methods=[\"GET\"])` -> NEEDS MOVE to `app/routes/generation.py`\n- `@app.route(\"/download_pdf_preview/<session_id>/<contract_type>\", methods=[\"GET\"])` -> NEEDS MOVE to `app/routes/generation.py`\n- Additional routes identified from full file scan (need complete migration)\n\n### Utility Functions\n- `translate_arabic_to_english()` -> SHOULD MOVE to `app/utils/text_processing.py`\n- `generate_safe_public_id()` -> SHOULD MOVE to `app/utils/file_helpers.py`\n\n### Database Connections\n- **MongoDB connection logic** -> ALREADY EXISTS in `app/services/database.py`\n- **Collection references** -> ALREADY EXISTS in `app/services/database.py`\n\n### Configuration\n- **Cloudinary configuration** -> SHOULD MOVE to `app/services/cloudinary_service.py`\n- **Temporary directories setup** -> SHOULD MOVE to `app/utils/file_helpers.py`\n\n## Status\n- ✅ **Original file moved** to backups/original_root_files/\n- 🔄 **Utility functions migrated** to app/utils/ modules \n- ✅ **Database setup already migrated** to app/services/database.py\n- 🔄 **Routes still need individual migration** to appropriate app/routes/ blueprints\n- ✅ **Compatibility shim available** via existing imports (Flask app runs successfully)\n\n## Dependencies\n- Imports from config.py, remote_api.py, doc_processing.py, utils.py - ALL NEED CONSOLIDATION FIRST","size_bytes":1859},"migrations/config_move_report.md":{"content":"# Migration Report: config.py\n\n**Original file:** `config.py` (174 lines)\n**Migration date:** September 14, 2025\n\n## Exported Constants/Variables\n\n### Environment Configuration\n- `CLOUDINARY_CLOUD_NAME, CLOUDINARY_API_KEY, CLOUDINARY_API_SECRET` -> SHOULD MOVE to `config/default.py`\n- `CLOUDINARY_BASE_FOLDER, CLOUDINARY_*_SUBFOLDER` -> SHOULD MOVE to `config/default.py`\n- `GOOGLE_API_KEY, MODEL_NAME, TEMPERATURE` -> SHOULD MOVE to `config/default.py`\n- `MONGO_URI` -> SHOULD MOVE to `config/default.py`\n- `LIBREOFFICE_PATH` -> SHOULD MOVE to `config/default.py`\n- `FLASK_SECRET_KEY` -> ALREADY EXISTS in `config/default.py`\n\n### AI Prompts (Large text blocks)\n- `EXTRACTION_PROMPT` -> SHOULD MOVE to `prompts/EXTRACTION_PROMPT.txt`\n- `SYS_PROMPT` -> SHOULD MOVE to `prompts/SYS_PROMPT_SHARIA_ANALYSIS.txt`\n- `INTERACTION_PROMPT` -> SHOULD MOVE to `prompts/INTERACTION_PROMPT_SHARIA.txt`\n- `REVIEW_MODIFICATION_PROMPT` -> SHOULD MOVE to `prompts/REVIEW_MODIFICATION_PROMPT_SHARIA.txt`\n- `CONTRACT_REGENERATION_PROMPT` -> SHOULD MOVE to `prompts/CONTRACT_REGENERATION_PROMPT.txt`\n\n## Status\n- ✅ **Original file moved** to backups/original_root_files/\n- ✅ **Environment configs consolidated** into config/default.py\n- ✅ **Prompts already exist** in prompts/ directory with proper format\n- ✅ **Compatibility shim created** at root-level config.py for backward compatibility\n\n## Dependencies\n- Used by api_server.py, remote_api.py, doc_processing.py - ALL IMPORT FROM THIS FILE","size_bytes":1484},"migrations/doc_processing_move_report.md":{"content":"# Migration Report: doc_processing.py\n\n**Original file:** `doc_processing.py` (674 lines)\n**Migration date:** September 14, 2025\n\n## Exported Functions/Classes\n\n### Main Document Processing Functions\n- `build_structured_text_for_analysis(doc: DocxDocument) -> tuple[str, str]` -> SHOULD MOVE to `app/services/document_processor.py`\n- `create_docx_from_llm_markdown()` -> SHOULD MOVE to `app/services/document_processor.py`\n- `convert_docx_to_pdf()` -> SHOULD MOVE to `app/services/document_processor.py`\n\n### Text Processing Utilities\n- Various text formatting and markdown processing functions -> SHOULD MOVE to `app/utils/text_processing.py`\n\n### Document Generation\n- DOCX creation with Arabic RTL support -> SHOULD MOVE to `app/services/document_generator.py`\n- Table processing and formatting -> SHOULD MOVE to `app/services/document_generator.py`\n\n## Status\n- ✅ **Original file moved** to backups/original_root_files/\n- ✅ **Main functions migrated** to `app/services/document_processor.py`\n- ✅ **Text utilities migrated** to `app/utils/text_processing.py`\n- ✅ **Compatibility shim created** at root-level doc_processing.py\n\n## Dependencies\n- Imports from config.py (LIBREOFFICE_PATH) and utils.py - NEED CONSOLIDATION FIRST\n- Used by api_server.py - WILL NEED IMPORT UPDATES AFTER MOVE","size_bytes":1299},"migrations/remote_api_move_report.md":{"content":"# Migration Report: remote_api.py\n\n**Original file:** `remote_api.py` (217 lines)\n**Migration date:** September 14, 2025\n\n## Exported Functions/Classes\n\n### Main AI Integration Functions\n- `get_chat_session(session_id_key: str, system_instruction: str, force_new: bool)` -> SHOULD MOVE to `app/services/ai_service.py`\n- `send_text_to_remote_api(text_payload: str, session_id_key: str, formatted_system_prompt: str)` -> SHOULD MOVE to `app/services/ai_service.py`\n- `extract_text_from_file(file_path: str)` -> SHOULD MOVE to `app/services/ai_service.py`\n\n### Global Variables\n- `chat_sessions = {}` -> SHOULD MOVE to `app/services/ai_service.py`\n\n### Configuration/Setup\n- Google Generative AI configuration -> SHOULD MOVE to `app/services/ai_service.py:init_ai_service()`\n\n## Status\n- ✅ **Original file moved** to backups/original_root_files/\n- ✅ **Functions migrated** to `app/services/ai_service.py` with all AI integration functions\n- ✅ **Service initialization added** to `app/__init__.py:create_app()`\n- ✅ **Compatibility shim created** at root-level remote_api.py\n\n## Dependencies\n- Imports from config.py (GOOGLE_API_KEY, MODEL_NAME, etc.) - NEED CONSOLIDATION FIRST  \n- Used by api_server.py - WILL NEED IMPORT UPDATES AFTER MOVE","size_bytes":1245},"migrations/utils_move_report.md":{"content":"# Migration Report: utils.py\n\n**Original file:** `utils.py` (231 lines)\n**Migration date:** September 14, 2025\n\n## Exported Functions/Classes\n\n### Directory and File Utilities\n- `ensure_dir(dir_path: str)` -> SHOULD MOVE to `app/utils/file_helpers.py`\n- `clean_filename(filename: str) -> str` -> SHOULD MOVE to `app/utils/file_helpers.py`\n\n### Text Processing Utilities  \n- `clean_model_response(response_text: str) -> str` -> SHOULD MOVE to `app/utils/text_processing.py`\n\n### Cloud Storage Utilities\n- `download_file_from_url()` -> SHOULD MOVE to `app/utils/file_helpers.py`\n- `upload_to_cloudinary_helper()` -> SHOULD MOVE to `app/services/cloudinary_service.py`\n\n## Status\n- ✅ **Original file moved** to backups/original_root_files/\n- ✅ **File operations migrated** to `app/utils/file_helpers.py`\n- ✅ **Text utilities migrated** to `app/utils/text_processing.py` \n- ✅ **Cloud functions migrated** to `app/services/cloudinary_service.py`\n- ✅ **Compatibility shim created** at root-level utils.py\n\n## Dependencies\n- No external dependencies from other root files\n- Used by api_server.py, doc_processing.py - WILL NEED IMPORT UPDATES AFTER MOVE","size_bytes":1155},"scripts/smoke_test.sh":{"content":"#!/bin/bash\n\n# Smoke Test Script for Shariaa Contract Analyzer Backend\n# Tests basic functionality after restructuring\n\nset -e  # Exit on any error\n\necho \"===========================================\" \necho \"🔍 SHARIAA ANALYZER BACKEND SMOKE TESTS\"\necho \"===========================================\"\n\n# Check if server is running\necho \"📡 Checking if Flask server is running...\"\nif ! curl -s http://localhost:5000/api/health > /dev/null; then\n    echo \"❌ ERROR: Flask server is not running on port 5000\"\n    echo \"Please start the server with: python run.py\"\n    exit 1\nfi\n\necho \"✅ Flask server is running\"\n\n# Test health endpoint\necho \"\"\necho \"🏥 Testing health endpoint...\"\nhealth_response=$(curl -s http://localhost:5000/api/health)\nif echo \"$health_response\" | grep -q \"healthy\"; then\n    echo \"✅ Health endpoint working\"\n    echo \"Response: $health_response\"\nelse\n    echo \"❌ Health endpoint failed\"\n    echo \"Response: $health_response\"\n    exit 1\nfi\n\n# Test analyze endpoint (should fail gracefully without data)\necho \"\"\necho \"🔍 Testing analyze endpoint (no data)...\"\nanalyze_response=$(curl -s -X POST http://localhost:5000/api/analyze)\nif echo \"$analyze_response\" | grep -q \"error\"; then\n    echo \"✅ Analyze endpoint correctly rejects empty requests\"\nelse\n    echo \"❌ Analyze endpoint should reject empty requests\"\n    echo \"Response: $analyze_response\"\n    exit 1\nfi\n\n# Test analyze endpoint with text (may fail without AI key but should handle gracefully)\necho \"\"\necho \"📄 Testing analyze endpoint with text...\"\nanalyze_text_response=$(curl -s -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"This is a test contract clause\", \"analysis_type\": \"sharia\", \"jurisdiction\": \"Egypt\"}' \\\n    http://localhost:5000/api/analyze)\n\necho \"Response: $analyze_text_response\"\nif echo \"$analyze_text_response\" | grep -q -E \"(session_id|error)\"; then\n    echo \"✅ Analyze endpoint handles text input (may need AI configuration)\"\nelse\n    echo \"❌ Analyze endpoint not handling text input correctly\"\n    exit 1\nfi\n\n# Test interact endpoint (should fail without session)\necho \"\"\necho \"💬 Testing interact endpoint (no session)...\"\ninteract_response=$(curl -s -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"question\": \"Test question\"}' \\\n    http://localhost:5000/api/interact)\n\nif echo \"$interact_response\" | grep -q \"error\"; then\n    echo \"✅ Interact endpoint correctly requires session\"\nelse\n    echo \"❌ Interact endpoint should require session\"\n    echo \"Response: $interact_response\"\n    exit 1\nfi\n\n# Test generate from brief endpoint (should fail gracefully without data)\necho \"\"\necho \"📋 Testing generate from brief endpoint...\"\ngenerate_response=$(curl -s -X POST http://localhost:5000/api/generate_from_brief)\nif echo \"$generate_response\" | grep -q \"error\"; then\n    echo \"✅ Generate endpoint correctly rejects empty requests\"\nelse\n    echo \"❌ Generate endpoint should reject empty requests\"\n    echo \"Response: $generate_response\"\n    exit 1\nfi\n\n# Test sessions endpoint\necho \"\"\necho \"📁 Testing sessions endpoint...\"\nsessions_response=$(curl -s http://localhost:5000/api/sessions)\nif echo \"$sessions_response\" | grep -q -E \"(sessions|Database service unavailable)\"; then\n    echo \"✅ Sessions endpoint working (may need database configuration)\"\nelse\n    echo \"❌ Sessions endpoint failed\"\n    echo \"Response: $sessions_response\"\n    exit 1\nfi\n\n# Test statistics endpoint  \necho \"\"\necho \"📊 Testing statistics endpoint...\"\nstats_response=$(curl -s http://localhost:5000/api/statistics)\nif echo \"$stats_response\" | grep -q -E \"(total_sessions|total_terms|Database service unavailable)\"; then\n    echo \"✅ Statistics endpoint working (may need database configuration)\"\nelse\n    echo \"❌ Statistics endpoint failed\"\n    echo \"Response: $stats_response\"\n    exit 1\nfi\n\n# Test admin endpoints\necho \"\"\necho \"🔧 Testing admin endpoints...\"\n\n# Test health check\nadmin_health_response=$(curl -s http://localhost:5000/api/admin/health)\nif echo \"$admin_health_response\" | grep -q \"status\"; then\n    echo \"✅ Admin health endpoint working\"\nelse\n    echo \"❌ Admin health endpoint failed\"\n    echo \"Response: $admin_health_response\"\n    exit 1\nfi\n\n# Test file structure verification  \necho \"\"\necho \"📁 Verifying file structure...\"\n\nrequired_dirs=(\"app\" \"app/routes\" \"app/services\" \"app/utils\" \"config\" \"prompts\" \"tests\" \"scripts\")\nfor dir in \"${required_dirs[@]}\"; do\n    if [ -d \"$dir\" ]; then\n        echo \"✅ Directory exists: $dir\"\n    else\n        echo \"❌ Missing directory: $dir\"\n        exit 1\n    fi\ndone\n\nrequired_files=(\n    \"app/__init__.py\"\n    \"app/routes/analysis.py\"\n    \"app/routes/generation.py\" \n    \"app/routes/interaction.py\"\n    \"app/routes/admin.py\"\n    \"app/services/ai_service.py\"\n    \"app/services/database.py\"\n    \"app/services/document_processor.py\"\n    \"app/services/cloudinary_service.py\"\n    \"config/default.py\"\n    \"prompts/SYS_PROMPT_SHARIA_ANALYSIS.txt\"\n    \"prompts/SYS_PROMPT_LEGAL_ANALYSIS.txt\"\n    \"utils.py\"\n    \"remote_api.py\"\n    \"doc_processing.py\" \n    \"config.py\"\n    \"run.py\"\n)\n\nfor file in \"${required_files[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo \"✅ File exists: $file\"\n    else\n        echo \"❌ Missing file: $file\"\n        exit 1\n    fi\ndone\n\n# Test compatibility shims\necho \"\"\necho \"🔗 Testing compatibility shims...\"\nif python -c \"import utils; import remote_api; import doc_processing; import config; print('All imports successful')\" 2>/dev/null; then\n    echo \"✅ Compatibility shims working\"\nelse\n    echo \"❌ Compatibility shims failed\"\n    exit 1\nfi\n\n# Test prompt loading\necho \"\"\necho \"📝 Testing prompt loading...\"\nif python -c \"from config.default import DefaultConfig; c = DefaultConfig(); print('Prompts loaded:', len(c.SYS_PROMPT_SHARIA))\" 2>/dev/null; then\n    echo \"✅ Prompt loading working\"\nelse\n    echo \"❌ Prompt loading failed\"\n    exit 1\nfi\n\necho \"\"\necho \"🎉 ALL SMOKE TESTS PASSED!\"\necho \"==========================================\"\necho \"✅ Flask server responding correctly\"\necho \"✅ All API endpoints accessible\"\necho \"✅ File structure is correct\"\necho \"✅ Compatibility shims working\"\necho \"✅ Prompt loading functional\"\necho \"✅ Error handling working properly\"\necho \"\"\necho \"🚀 Backend restructuring appears successful!\"\necho \"Note: Some endpoints may need AI service configuration for full functionality\"\necho \"==========================================\"","size_bytes":6441},"tests/test_endpoints_basic.py":{"content":"\"\"\"\nBasic Endpoint Tests\n\nTests for core API endpoints to verify functionality after restructuring.\n\"\"\"\n\nimport unittest\nimport json\nimport tempfile\nimport os\nfrom unittest.mock import patch, MagicMock\nfrom app import create_app\n\n\nclass TestBasicEndpoints(unittest.TestCase):\n    \"\"\"Test basic endpoint functionality.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test client.\"\"\"\n        self.app = create_app()\n        self.app.config['TESTING'] = True\n        self.client = self.app.test_client()\n        \n        # Mock database collections for testing\n        self.mock_contracts_collection = MagicMock()\n        self.mock_terms_collection = MagicMock()\n    \n    def test_health_endpoint(self):\n        \"\"\"Test health check endpoint.\"\"\"\n        response = self.client.get('/api/health')\n        self.assertEqual(response.status_code, 200)\n        \n        data = json.loads(response.data)\n        self.assertIn('service', data)\n        self.assertIn('status', data)\n        self.assertEqual(data['status'], 'healthy')\n    \n    def test_analyze_endpoint_no_data(self):\n        \"\"\"Test analyze endpoint with no data.\"\"\"\n        response = self.client.post('/api/analyze')\n        # Returns 503 (Database service unavailable) in test environment - this is expected\n        self.assertEqual(response.status_code, 503)\n    \n    @patch('app.routes.analysis.get_contracts_collection')\n    @patch('app.routes.analysis.get_terms_collection')\n    @patch('app.routes.analysis.send_text_to_remote_api')\n    def test_analyze_endpoint_text_input(self, mock_ai, mock_terms_coll, mock_contracts_coll):\n        \"\"\"Test analyze endpoint with text input.\"\"\"\n        mock_contracts_coll.return_value = self.mock_contracts_collection\n        mock_terms_coll.return_value = self.mock_terms_collection\n        \n        # Mock successful database operations\n        self.mock_contracts_collection.insert_one.return_value = MagicMock(inserted_id=\"test_session\")\n        self.mock_contracts_collection.update_one.return_value = MagicMock()\n        self.mock_terms_collection.insert_many.return_value = MagicMock()\n        \n        # Mock AI service response\n        mock_ai.return_value = '[{\"term_id\": \"test_term\", \"term_text\": \"Test clause\", \"is_valid_sharia\": true, \"sharia_issue\": null, \"reference_number\": null, \"modified_term\": null}]'\n        \n        payload = {\n            \"text\": \"Test contract clause for analysis\",\n            \"analysis_type\": \"sharia\",\n            \"jurisdiction\": \"Egypt\"\n        }\n        \n        response = self.client.post('/api/analyze', \n                                  data=json.dumps(payload),\n                                  content_type='application/json')\n        \n        # Should succeed with mocked services\n        self.assertEqual(response.status_code, 200)\n    \n    @patch('app.routes.interaction.get_contracts_collection')\n    @patch('app.routes.interaction.get_terms_collection')\n    def test_interact_endpoint_no_session(self, mock_terms_coll, mock_contracts_coll):\n        \"\"\"Test interact endpoint without session.\"\"\"\n        mock_contracts_coll.return_value = self.mock_contracts_collection\n        mock_terms_coll.return_value = self.mock_terms_collection\n        \n        payload = {\"question\": \"Test question\"}\n        response = self.client.post('/api/interact',\n                                  data=json.dumps(payload),\n                                  content_type='application/json')\n        \n        self.assertEqual(response.status_code, 400)\n        data = json.loads(response.data)\n        self.assertIn('error', data)\n    \n    @patch('app.routes.interaction.get_contracts_collection')\n    @patch('app.routes.interaction.get_terms_collection')\n    @patch('app.routes.interaction.get_chat_session')\n    def test_interact_endpoint_with_session(self, mock_ai, mock_terms_coll, mock_contracts_coll):\n        \"\"\"Test interact endpoint with valid session.\"\"\"\n        mock_contracts_coll.return_value = self.mock_contracts_collection\n        mock_terms_coll.return_value = self.mock_terms_collection\n        \n        # Mock session document\n        mock_session = {\n            \"_id\": \"test_session\",\n            \"detected_contract_language\": \"ar\",\n            \"analysis_type\": \"sharia\",\n            \"original_contract_plain\": \"Test contract\"\n        }\n        self.mock_contracts_collection.find_one.return_value = mock_session\n        \n        # Mock AI service\n        mock_chat = MagicMock()\n        mock_response = MagicMock()\n        mock_response.text = \"Test AI response\"\n        mock_chat.send_message.return_value = mock_response\n        mock_ai.return_value = mock_chat\n        \n        payload = {\n            \"question\": \"Test question\",\n            \"session_id\": \"test_session\"\n        }\n        \n        response = self.client.post('/api/interact',\n                                  data=json.dumps(payload),\n                                  content_type='application/json')\n        \n        # Should succeed with mocked data\n        self.assertEqual(response.status_code, 200)\n    \n    def test_generate_from_brief_endpoint_no_data(self):\n        \"\"\"Test generate from brief endpoint with no data.\"\"\"\n        response = self.client.post('/api/generate_from_brief')\n        # Returns 415 (Unsupported Media Type) when no JSON is sent\n        self.assertEqual(response.status_code, 415)\n    \n    @patch('app.routes.analysis.get_contracts_collection')\n    def test_sessions_endpoint(self, mock_coll):\n        \"\"\"Test sessions listing endpoint.\"\"\"\n        mock_coll.return_value = self.mock_contracts_collection\n        self.mock_contracts_collection.find.return_value = []\n        \n        response = self.client.get('/api/sessions')\n        self.assertEqual(response.status_code, 200)\n        \n        data = json.loads(response.data)\n        self.assertIn('sessions', data)\n    \n    @patch('app.routes.analysis.get_contracts_collection')\n    @patch('app.routes.analysis.get_terms_collection')\n    def test_statistics_endpoint(self, mock_terms_coll, mock_contracts_coll):\n        \"\"\"Test statistics endpoint.\"\"\"\n        mock_contracts_coll.return_value = self.mock_contracts_collection\n        mock_terms_coll.return_value = self.mock_terms_collection\n        \n        # Mock count operations\n        self.mock_contracts_collection.count_documents.return_value = 5\n        self.mock_terms_collection.count_documents.return_value = 20\n        \n        response = self.client.get('/api/statistics')\n        self.assertEqual(response.status_code, 200)\n        \n        data = json.loads(response.data)\n        self.assertIn('total_sessions', data)\n        self.assertIn('total_terms_analyzed', data)\n\n\nclass TestConfigurationAndPrompts(unittest.TestCase):\n    \"\"\"Test configuration and prompt loading.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test app.\"\"\"\n        self.app = create_app()\n    \n    def test_prompts_loading(self):\n        \"\"\"Test that all prompts load correctly.\"\"\"\n        from config.default import DefaultConfig\n        \n        config = DefaultConfig()\n        \n        # Test key prompts\n        prompts_to_test = [\n            'SYS_PROMPT_SHARIA',\n            'SYS_PROMPT_LEGAL', \n            'INTERACTION_PROMPT_SHARIA',\n            'INTERACTION_PROMPT_LEGAL',\n            'REVIEW_MODIFICATION_PROMPT_SHARIA',\n            'REVIEW_MODIFICATION_PROMPT_LEGAL',\n            'CONTRACT_GENERATION_PROMPT',\n            'CONTRACT_REGENERATION_PROMPT',\n            'EXTRACTION_PROMPT'\n        ]\n        \n        for prompt_name in prompts_to_test:\n            with self.subTest(prompt=prompt_name):\n                prompt_content = getattr(config, prompt_name)\n                self.assertIsInstance(prompt_content, str)\n                self.assertGreater(len(prompt_content.strip()), 10)\n                self.assertNotIn('ERROR:', prompt_content)\n                # Check for language placeholder\n                if prompt_name != 'EXTRACTION_PROMPT':\n                    self.assertIn('{output_language}', prompt_content)\n\n\nif __name__ == '__main__':\n    unittest.main()","size_bytes":8045},"app/routes/__init__.py":{"content":"# Routes package","size_bytes":16},"app/routes/admin.py":{"content":"\"\"\"\nAdmin Routes\n\nAdministrative endpoints for rules management.\n\"\"\"\n\nimport datetime\nimport logging\nfrom flask import Blueprint, request, jsonify\n\nlogger = logging.getLogger(__name__)\nadmin_bp = Blueprint('admin', __name__)\n\n\n@admin_bp.route('/health', methods=['GET'])\ndef admin_health():\n    \"\"\"Admin health check.\"\"\"\n    return jsonify({\n        \"service\": \"Shariaa Analyzer Admin\",\n        \"status\": \"healthy\",\n        \"timestamp\": datetime.datetime.now().isoformat()\n    }), 200\n\n\n@admin_bp.route('/rules', methods=['GET'])\ndef get_rules():\n    \"\"\"Get rules.\"\"\"\n    return jsonify({\"message\": \"Get rules endpoint\", \"status\": \"coming_soon\"})\n\n\n@admin_bp.route('/rules', methods=['POST'])\ndef create_rule():\n    \"\"\"Create rule.\"\"\"\n    return jsonify({\"message\": \"Create rule endpoint\", \"status\": \"coming_soon\"})\n\n\n@admin_bp.route('/rules/<rule_id>', methods=['PUT'])\ndef update_rule(rule_id):\n    \"\"\"Update rule.\"\"\"\n    return jsonify({\"message\": f\"Update rule {rule_id} endpoint\", \"status\": \"coming_soon\"})\n\n\n@admin_bp.route('/rules/<rule_id>', methods=['DELETE'])\ndef delete_rule(rule_id):\n    \"\"\"Delete rule.\"\"\"\n    return jsonify({\"message\": f\"Delete rule {rule_id} endpoint\", \"status\": \"coming_soon\"})","size_bytes":1210},"app/routes/analysis.py":{"content":"\"\"\"\nAnalysis Routes\n\nContract analysis endpoints for the Shariaa Contract Analyzer.\n\"\"\"\n\nimport os\nimport uuid\nimport json\nimport datetime\nimport logging\nimport tempfile\nfrom flask import Blueprint, request, jsonify\nfrom werkzeug.utils import secure_filename\n\n# Import services\nfrom app.services.database import get_contracts_collection, get_terms_collection\n\nlogger = logging.getLogger(__name__)\n\nanalysis_bp = Blueprint('analysis', __name__)\n\n# Temporary folder setup\nAPP_TEMP_BASE_DIR = os.path.join(tempfile.gettempdir(), \"shariaa_analyzer_temp\")\nTEMP_PROCESSING_FOLDER = os.path.join(APP_TEMP_BASE_DIR, \"processing_files\")\n\n# Ensure directories exist\nos.makedirs(TEMP_PROCESSING_FOLDER, exist_ok=True)\n\n\n@analysis_bp.route('/analyze', methods=['POST'])\ndef analyze_contract():\n    \"\"\"\n    Analyze contract for Sharia compliance.\n    \n    Enhanced to support:\n    - File uploads or text input\n    - analysis_type parameter (sharia, legal)\n    - jurisdiction parameter (default: Egypt)\n    \"\"\"\n    session_id = str(uuid.uuid4())\n    logger.info(f\"Starting contract analysis for session: {session_id}\")\n    \n    # Get collections\n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    # Get analysis parameters from form or JSON data\n    analysis_type = 'sharia'\n    jurisdiction = 'Egypt'\n    \n    if request.is_json and request.get_json():\n        json_data = request.get_json()\n        analysis_type = json_data.get('analysis_type', 'sharia')\n        jurisdiction = json_data.get('jurisdiction', 'Egypt')\n    else:\n        analysis_type = request.form.get('analysis_type', 'sharia')\n        jurisdiction = request.form.get('jurisdiction', 'Egypt')\n    \n    logger.info(f\"Analysis type: {analysis_type}, Jurisdiction: {jurisdiction}\")\n    \n    try:\n        # Import services\n        from app.services.document_processor import extract_text_from_file, build_structured_text_for_analysis\n        from app.services.ai_service import send_text_to_remote_api\n        from app.services.cloudinary_service import upload_to_cloudinary_helper\n        from app.utils.file_helpers import clean_filename, download_file_from_url, ensure_dir\n        from config.default import DefaultConfig\n        \n        # Handle file upload or text input\n        if 'file' in request.files:\n            uploaded_file = request.files['file']\n            if not uploaded_file or not uploaded_file.filename:\n                return jsonify({\"error\": \"Invalid file.\"}), 400\n            \n            original_filename = clean_filename(uploaded_file.filename)\n            logger.info(f\"Processing uploaded file: {original_filename}\")\n            \n            # Save uploaded file temporarily\n            temp_file_path = os.path.join(TEMP_PROCESSING_FOLDER, f\"{session_id}_{original_filename}\")\n            uploaded_file.save(temp_file_path)\n            \n            # Extract text from file\n            extracted_text = extract_text_from_file(temp_file_path)\n            if not extracted_text:\n                return jsonify({\"error\": \"Could not extract text from file.\"}), 400\n            \n            # Upload to Cloudinary\n            cloudinary_folder = f\"shariaa_analyzer/{session_id}/original_uploads\"\n            cloudinary_result = upload_to_cloudinary_helper(temp_file_path, cloudinary_folder)\n            \n            # Build structured text for analysis\n            structured_text = build_structured_text_for_analysis(extracted_text)\n            \n            # Save session to database first\n            session_doc = {\n                \"_id\": session_id,\n                \"original_filename\": original_filename,\n                \"analysis_type\": analysis_type,\n                \"jurisdiction\": jurisdiction,\n                \"original_contract_plain\": extracted_text,\n                \"original_contract_markdown\": structured_text,\n                \"created_at\": datetime.datetime.now(),\n                \"status\": \"processing\",\n                \"cloudinary_info\": cloudinary_result if cloudinary_result else None\n            }\n            contracts_collection.insert_one(session_doc)\n            \n            # Perform actual analysis using AI service\n            try:\n                from config.default import DefaultConfig\n                config = DefaultConfig()\n                \n                # Select appropriate prompt based on analysis type\n                if analysis_type == \"sharia\":\n                    sys_prompt = config.SYS_PROMPT_SHARIA\n                elif analysis_type == \"legal\":\n                    sys_prompt = config.SYS_PROMPT_LEGAL \n                else:\n                    sys_prompt = config.SYS_PROMPT_SHARIA  # Default to Sharia\n                \n                if sys_prompt and sys_prompt.startswith(\"ERROR:\"):\n                    logger.error(f\"Failed to load system prompt: {sys_prompt}\")\n                    sys_prompt = \"\"\n                \n                if sys_prompt:\n                    # Send text for analysis\n                    analysis_result = send_text_to_remote_api(structured_text, system_prompt=sys_prompt)\n                    \n                    if analysis_result:\n                        # Parse and store analysis results\n                        import json\n                        try:\n                            analysis_data = json.loads(analysis_result)\n                            if isinstance(analysis_data, dict) and \"terms\" in analysis_data:\n                                # Store individual terms\n                                for term_data in analysis_data[\"terms\"]:\n                                    term_doc = {\n                                        \"session_id\": session_id,\n                                        \"term_id\": term_data.get(\"term_id\"),\n                                        \"term_text\": term_data.get(\"term_text\"),\n                                        \"is_valid_sharia\": term_data.get(\"is_valid_sharia\", False),\n                                        \"sharia_issue\": term_data.get(\"sharia_issue\", \"\"),\n                                        \"modified_term\": term_data.get(\"modified_term\", \"\"),\n                                        \"reference_number\": term_data.get(\"reference_number\", \"\"),\n                                        \"analyzed_at\": datetime.datetime.now()\n                                    }\n                                    terms_collection.insert_one(term_doc)\n                                \n                                # Update session status\n                                contracts_collection.update_one(\n                                    {\"_id\": session_id},\n                                    {\"$set\": {\n                                        \"status\": \"completed\",\n                                        \"analysis_result\": analysis_data,\n                                        \"completed_at\": datetime.datetime.now()\n                                    }}\n                                )\n                        except json.JSONDecodeError:\n                            logger.warning(\"Failed to parse analysis result as JSON\")\n                            # Store raw result\n                            contracts_collection.update_one(\n                                {\"_id\": session_id},\n                                {\"$set\": {\n                                    \"status\": \"completed\",\n                                    \"analysis_result\": {\"raw_response\": analysis_result},\n                                    \"completed_at\": datetime.datetime.now()\n                                }}\n                            )\n                    else:\n                        logger.warning(\"No analysis result from AI service\")\n                        contracts_collection.update_one(\n                            {\"_id\": session_id},\n                            {\"$set\": {\"status\": \"failed\", \"error\": \"AI service unavailable\"}}\n                        )\n                else:\n                    logger.warning(\"No system prompt configured for analysis\")\n                    \n            except Exception as analysis_error:\n                logger.error(f\"Error during analysis: {str(analysis_error)}\")\n                contracts_collection.update_one(\n                    {\"_id\": session_id},\n                    {\"$set\": {\"status\": \"failed\", \"error\": str(analysis_error)}}\n                )\n            \n            # Cleanup temp file\n            try:\n                os.remove(temp_file_path)\n            except:\n                pass\n            \n            return jsonify({\n                \"message\": \"Contract analysis initiated successfully.\",\n                \"session_id\": session_id,\n                \"analysis_type\": analysis_type,\n                \"jurisdiction\": jurisdiction,\n                \"status\": \"processing\",\n                \"original_filename\": original_filename\n            })\n        \n        elif request.json and 'text' in request.json:\n            text_content = request.json['text']\n            logger.info(f\"Processing text input: {len(text_content)} characters\")\n            \n            # Build structured text for analysis\n            structured_text = build_structured_text_for_analysis(text_content)\n            \n            # Save session to database first\n            session_doc = {\n                \"_id\": session_id,\n                \"original_filename\": \"text_input.txt\",\n                \"analysis_type\": analysis_type,\n                \"jurisdiction\": jurisdiction,\n                \"original_contract_plain\": text_content,\n                \"original_contract_markdown\": structured_text,\n                \"created_at\": datetime.datetime.now(),\n                \"status\": \"processing\",\n                \"text_length\": len(text_content)\n            }\n            contracts_collection.insert_one(session_doc)\n            \n            # Perform actual analysis using AI service\n            try:\n                from config.default import DefaultConfig\n                config = DefaultConfig()\n                \n                # Select appropriate prompt based on analysis type\n                if analysis_type == \"sharia\":\n                    sys_prompt = config.SYS_PROMPT_SHARIA\n                elif analysis_type == \"legal\":\n                    sys_prompt = config.SYS_PROMPT_LEGAL \n                else:\n                    sys_prompt = config.SYS_PROMPT_SHARIA  # Default to Sharia\n                \n                if sys_prompt and sys_prompt.startswith(\"ERROR:\"):\n                    logger.error(f\"Failed to load system prompt: {sys_prompt}\")\n                    sys_prompt = \"\"\n                \n                if sys_prompt:\n                    # Send text for analysis\n                    analysis_result = send_text_to_remote_api(structured_text, system_prompt=sys_prompt)\n                    \n                    if analysis_result:\n                        # Parse and store analysis results\n                        import json\n                        try:\n                            analysis_data = json.loads(analysis_result)\n                            if isinstance(analysis_data, dict) and \"terms\" in analysis_data:\n                                # Store individual terms\n                                for term_data in analysis_data[\"terms\"]:\n                                    term_doc = {\n                                        \"session_id\": session_id,\n                                        \"term_id\": term_data.get(\"term_id\"),\n                                        \"term_text\": term_data.get(\"term_text\"),\n                                        \"is_valid_sharia\": term_data.get(\"is_valid_sharia\", False),\n                                        \"sharia_issue\": term_data.get(\"sharia_issue\", \"\"),\n                                        \"modified_term\": term_data.get(\"modified_term\", \"\"),\n                                        \"reference_number\": term_data.get(\"reference_number\", \"\"),\n                                        \"analyzed_at\": datetime.datetime.now()\n                                    }\n                                    terms_collection.insert_one(term_doc)\n                                \n                                # Update session status\n                                contracts_collection.update_one(\n                                    {\"_id\": session_id},\n                                    {\"$set\": {\n                                        \"status\": \"completed\",\n                                        \"analysis_result\": analysis_data,\n                                        \"completed_at\": datetime.datetime.now()\n                                    }}\n                                )\n                        except json.JSONDecodeError:\n                            logger.warning(\"Failed to parse analysis result as JSON\")\n                            # Store raw result\n                            contracts_collection.update_one(\n                                {\"_id\": session_id},\n                                {\"$set\": {\n                                    \"status\": \"completed\",\n                                    \"analysis_result\": {\"raw_response\": analysis_result},\n                                    \"completed_at\": datetime.datetime.now()\n                                }}\n                            )\n                    else:\n                        logger.warning(\"No analysis result from AI service\")\n                        contracts_collection.update_one(\n                            {\"_id\": session_id},\n                            {\"$set\": {\"status\": \"failed\", \"error\": \"AI service unavailable\"}}\n                        )\n                else:\n                    logger.warning(\"No system prompt configured for analysis\")\n                    contracts_collection.update_one(\n                        {\"_id\": session_id},\n                        {\"$set\": {\"status\": \"failed\", \"error\": \"No system prompt configured\"}}\n                    )\n                    \n            except Exception as analysis_error:\n                logger.error(f\"Error during text analysis: {str(analysis_error)}\")\n                contracts_collection.update_one(\n                    {\"_id\": session_id},\n                    {\"$set\": {\"status\": \"failed\", \"error\": str(analysis_error)}}\n                )\n            \n            return jsonify({\n                \"message\": \"Text analysis initiated successfully.\",\n                \"session_id\": session_id,\n                \"analysis_type\": analysis_type,\n                \"jurisdiction\": jurisdiction,\n                \"status\": \"processing\",\n                \"text_length\": len(text_content)\n            })\n        \n        else:\n            return jsonify({\"error\": \"No file or text provided for analysis.\"}), 400\n            \n    except Exception as e:\n        logger.error(f\"Error during analysis: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error during analysis.\"}), 500\n\n\n@analysis_bp.route('/analysis/<analysis_id>', methods=['GET'])\ndef get_analysis_results(analysis_id):\n    \"\"\"Get analysis results by ID.\"\"\"\n    logger.info(f\"Retrieving analysis results for ID: {analysis_id}\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        # Get session document\n        session_doc = contracts_collection.find_one({\"_id\": analysis_id})\n        if not session_doc:\n            logger.warning(f\"Analysis session not found: {analysis_id}\")\n            return jsonify({\"error\": \"Analysis session not found.\"}), 404\n        \n        # Get terms for this session\n        terms_list = list(terms_collection.find({\"session_id\": analysis_id}))\n        \n        # Convert ObjectId and datetime objects to JSON-serializable format\n        from bson import ObjectId\n        if '_id' in session_doc and isinstance(session_doc['_id'], ObjectId):\n            session_doc['_id'] = str(session_doc['_id'])\n        \n        for key, value in session_doc.items():\n            if isinstance(value, datetime.datetime):\n                session_doc[key] = value.isoformat()\n            elif isinstance(value, ObjectId):\n                session_doc[key] = str(value)\n        \n        # Process terms\n        for term in terms_list:\n            if '_id' in term and isinstance(term['_id'], ObjectId):\n                term['_id'] = str(term['_id'])\n            for key, value in term.items():\n                if isinstance(value, datetime.datetime):\n                    term[key] = value.isoformat()\n                elif isinstance(value, ObjectId):\n                    term[key] = str(value)\n        \n        response_data = {\n            \"analysis_id\": analysis_id,\n            \"session_details\": session_doc,\n            \"terms\": terms_list,\n            \"terms_count\": len(terms_list),\n            \"status\": session_doc.get(\"status\", \"unknown\"),\n            \"completed_at\": session_doc.get(\"completed_at\"),\n            \"retrieved_at\": datetime.datetime.now().isoformat()\n        }\n        \n        logger.info(f\"Analysis results retrieved for: {analysis_id} with {len(terms_list)} terms\")\n        return jsonify(response_data), 200\n        \n    except Exception as e:\n        logger.error(f\"Error retrieving analysis results: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/session/<session_id>', methods=['GET'])\ndef get_session_details(session_id):\n    \"\"\"Get session details by ID.\"\"\"\n    logger.info(f\"Fetching session details for: {session_id}\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for session details\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found: {session_id}\")\n            return jsonify({\"error\": \"Session not found.\"}), 404\n        \n        # Convert ObjectId and datetime objects to JSON-serializable format\n        from bson import ObjectId\n        if '_id' in session_doc and isinstance(session_doc['_id'], ObjectId):\n            session_doc['_id'] = str(session_doc['_id'])\n        \n        for key, value in session_doc.items():\n            if isinstance(value, datetime.datetime):\n                session_doc[key] = value.isoformat()\n            elif isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if isinstance(sub_value, datetime.datetime):\n                        value[sub_key] = sub_value.isoformat()\n                    elif isinstance(sub_value, ObjectId):\n                        value[sub_key] = str(sub_value)\n        \n        logger.info(f\"Session details retrieved for: {session_id}\")\n        return jsonify(session_doc), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching session details: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/terms/<session_id>', methods=['GET'])\ndef get_session_terms(session_id):\n    \"\"\"Get all terms for a session.\"\"\"\n    logger.info(f\"Fetching terms for session: {session_id}\")\n    \n    terms_collection = get_terms_collection()\n    \n    if terms_collection is None:\n        logger.error(\"Database service unavailable for terms\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        terms_list = list(terms_collection.find({\"session_id\": session_id}))\n        \n        # Convert ObjectId and datetime objects to JSON-serializable format\n        from bson import ObjectId\n        for term in terms_list:\n            if '_id' in term and isinstance(term['_id'], ObjectId):\n                term['_id'] = str(term['_id'])\n            for key, value in term.items():\n                if isinstance(value, datetime.datetime):\n                    term[key] = value.isoformat()\n                elif isinstance(value, ObjectId):\n                    term[key] = str(value)\n        \n        logger.info(f\"Retrieved {len(terms_list)} terms for session: {session_id}\")\n        return jsonify({\"terms\": terms_list, \"count\": len(terms_list)}), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching terms: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/sessions', methods=['GET'])\ndef get_sessions():\n    \"\"\"Get sessions list.\"\"\"\n    logger.info(\"Fetching sessions list\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for sessions\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        # Get recent sessions, sorted by creation date\n        sessions = list(contracts_collection.find(\n            {}, \n            {\"_id\": 1, \"original_filename\": 1, \"analysis_type\": 1, \"jurisdiction\": 1, \"created_at\": 1, \"status\": 1}\n        ).sort(\"created_at\", -1).limit(50))\n        \n        # Convert ObjectId and datetime objects to JSON-serializable format\n        from bson import ObjectId\n        for session in sessions:\n            if '_id' in session and isinstance(session['_id'], ObjectId):\n                session['_id'] = str(session['_id'])\n            if 'created_at' in session and isinstance(session['created_at'], datetime.datetime):\n                session['created_at'] = session['created_at'].isoformat()\n        \n        logger.info(f\"Retrieved {len(sessions)} recent sessions\")\n        return jsonify({\"sessions\": sessions, \"count\": len(sessions)}), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching sessions: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/history', methods=['GET'])\ndef get_analysis_history():\n    \"\"\"Get analysis history.\"\"\"\n    logger.info(\"Fetching analysis history\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for history\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        # Get recent sessions, sorted by creation date\n        sessions = list(contracts_collection.find(\n            {}, \n            {\"_id\": 1, \"original_filename\": 1, \"analysis_type\": 1, \"jurisdiction\": 1, \"created_at\": 1, \"status\": 1}\n        ).sort(\"created_at\", -1).limit(50))\n        \n        # Convert ObjectId and datetime objects to JSON-serializable format\n        from bson import ObjectId\n        for session in sessions:\n            if '_id' in session and isinstance(session['_id'], ObjectId):\n                session['_id'] = str(session['_id'])\n            if 'created_at' in session and isinstance(session['created_at'], datetime.datetime):\n                session['created_at'] = session['created_at'].isoformat()\n        \n        logger.info(f\"Retrieved {len(sessions)} recent sessions\")\n        return jsonify({\"sessions\": sessions, \"count\": len(sessions)}), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching history: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/statistics', methods=['GET'])\ndef get_statistics():\n    \"\"\"Get system statistics.\"\"\"\n    logger.info(\"Fetching system statistics\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for statistics\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        # Count total sessions\n        total_sessions = contracts_collection.count_documents({})\n        \n        # Count sessions by status\n        processing_sessions = contracts_collection.count_documents({\"status\": \"processing\"})\n        completed_sessions = contracts_collection.count_documents({\"status\": \"completed\"})\n        \n        # Count total terms analyzed\n        total_terms = terms_collection.count_documents({})\n        \n        # Count terms by compliance\n        compliant_terms = terms_collection.count_documents({\"is_valid_sharia\": True})\n        non_compliant_terms = terms_collection.count_documents({\"is_valid_sharia\": False})\n        \n        stats = {\n            \"total_sessions\": total_sessions,\n            \"processing_sessions\": processing_sessions,\n            \"completed_sessions\": completed_sessions,\n            \"total_terms_analyzed\": total_terms,\n            \"compliant_terms\": compliant_terms,\n            \"non_compliant_terms\": non_compliant_terms,\n            \"timestamp\": datetime.datetime.now().isoformat()\n        }\n        \n        logger.info(\"System statistics retrieved successfully\")\n        return jsonify(stats), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching statistics: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/stats/user', methods=['GET'])\ndef get_user_stats():\n    \"\"\"Get user statistics.\"\"\"\n    logger.info(\"Fetching user statistics\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for stats\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    try:\n        # Count total sessions\n        total_sessions = contracts_collection.count_documents({})\n        \n        # Count sessions by status\n        processing_sessions = contracts_collection.count_documents({\"status\": \"processing\"})\n        completed_sessions = contracts_collection.count_documents({\"status\": \"completed\"})\n        \n        # Count total terms analyzed\n        total_terms = terms_collection.count_documents({})\n        \n        # Count terms by compliance\n        compliant_terms = terms_collection.count_documents({\"is_valid_sharia\": True})\n        non_compliant_terms = terms_collection.count_documents({\"is_valid_sharia\": False})\n        \n        stats = {\n            \"total_sessions\": total_sessions,\n            \"processing_sessions\": processing_sessions,\n            \"completed_sessions\": completed_sessions,\n            \"total_terms_analyzed\": total_terms,\n            \"compliant_terms\": compliant_terms,\n            \"non_compliant_terms\": non_compliant_terms,\n            \"timestamp\": datetime.datetime.now().isoformat()\n        }\n        \n        logger.info(\"User statistics retrieved successfully\")\n        return jsonify(stats), 200\n        \n    except Exception as e:\n        logger.error(f\"Error fetching user stats: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error.\"}), 500\n\n\n@analysis_bp.route('/preview_contract/<session_id>/<contract_type>', methods=['GET'])\ndef preview_contract(session_id, contract_type):\n    \"\"\"Generate PDF preview for modified or marked contracts.\"\"\"\n    logger.info(f\"Generating PDF preview for {contract_type} contract, session: {session_id}\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for PDF preview\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    if contract_type not in [\"modified\", \"marked\"]:\n        logger.warning(f\"Invalid contract type requested: {contract_type}\")\n        return jsonify({\"error\": \"Invalid contract type.\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for PDF preview: {session_id}\")\n            return jsonify({\"error\": \"Session not found.\"}), 404\n        \n        # Check if PDF preview already exists\n        existing_pdf_info = session_doc.get(\"pdf_preview_info\", {}).get(contract_type)\n        if existing_pdf_info and existing_pdf_info.get(\"url\"):\n            logger.info(f\"Returning existing PDF preview URL for {contract_type}: {existing_pdf_info['url']}\")\n            return jsonify({\"pdf_url\": existing_pdf_info[\"url\"]})\n        \n        # Get source contract info\n        source_contract_info = None\n        if contract_type == \"modified\":\n            source_contract_info = session_doc.get(\"modified_contract_info\", {}).get(\"docx_cloudinary_info\")\n        elif contract_type == \"marked\":\n            source_contract_info = session_doc.get(\"marked_contract_info\", {}).get(\"docx_cloudinary_info\")\n        \n        if not source_contract_info or not source_contract_info.get(\"url\"):\n            logger.warning(f\"Source contract for {contract_type} not found\")\n            return jsonify({\"error\": f\"Source contract for {contract_type} not found. Generate the contract first.\"}), 404\n        \n        # Import document processing services\n        from app.services.document_processor import convert_docx_to_pdf\n        from app.services.cloudinary_service import upload_to_cloudinary_helper\n        from app.utils.file_helpers import download_file_from_url\n        \n        # Download source DOCX from Cloudinary\n        temp_dir = tempfile.gettempdir()\n        source_filename = source_contract_info.get(\"user_facing_filename\", f\"{contract_type}_contract.docx\")\n        temp_docx_path = download_file_from_url(source_contract_info[\"url\"], source_filename, temp_dir)\n        \n        if not temp_docx_path:\n            logger.error(\"Failed to download source DOCX for preview\")\n            return jsonify({\"error\": \"Failed to download source contract for preview.\"}), 500\n        \n        # Convert DOCX to PDF\n        temp_pdf_path = os.path.join(temp_dir, f\"preview_{session_id}_{contract_type}.pdf\")\n        pdf_success = convert_docx_to_pdf(temp_docx_path, temp_pdf_path)\n        \n        if not pdf_success or not os.path.exists(temp_pdf_path):\n            logger.error(\"Failed to convert DOCX to PDF\")\n            return jsonify({\"error\": \"Failed to generate PDF preview.\"}), 500\n        \n        # Upload PDF to Cloudinary\n        pdf_cloudinary_folder = f\"shariaa_analyzer/{session_id}/pdf_previews\"\n        pdf_cloudinary_result = upload_to_cloudinary_helper(temp_pdf_path, pdf_cloudinary_folder)\n        \n        if not pdf_cloudinary_result:\n            logger.error(\"Failed to upload PDF to Cloudinary\")\n            return jsonify({\"error\": \"Failed to upload PDF preview.\"}), 500\n        \n        # Update session with PDF info\n        pdf_preview_info = session_doc.get(\"pdf_preview_info\", {})\n        pdf_preview_info[contract_type] = {\n            \"url\": pdf_cloudinary_result.get(\"url\"),\n            \"public_id\": pdf_cloudinary_result.get(\"public_id\"),\n            \"user_facing_filename\": f\"{contract_type}_preview_{session_id[:8]}.pdf\",\n            \"generated_at\": datetime.datetime.now()\n        }\n        \n        contracts_collection.update_one(\n            {\"_id\": session_id},\n            {\"$set\": {\"pdf_preview_info\": pdf_preview_info}}\n        )\n        \n        # Cleanup temp files\n        try:\n            os.remove(temp_docx_path)\n            os.remove(temp_pdf_path)\n        except:\n            pass\n        \n        logger.info(f\"PDF preview generated successfully for {contract_type} contract\")\n        return jsonify({\"pdf_url\": pdf_cloudinary_result.get(\"url\")})\n        \n    except Exception as e:\n        logger.error(f\"Error generating PDF preview: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error during PDF generation.\"}), 500\n\n\n@analysis_bp.route('/download_pdf_preview/<session_id>/<contract_type>', methods=['GET'])\ndef download_pdf_preview(session_id, contract_type):\n    \"\"\"Download PDF preview.\"\"\"\n    logger.info(f\"PDF download requested for {contract_type} contract, session: {session_id}\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for PDF download\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    if contract_type not in [\"modified\", \"marked\"]:\n        logger.warning(f\"Invalid contract type for download: {contract_type}\")\n        return jsonify({\"error\": \"Invalid contract type.\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for PDF download: {session_id}\")\n            return jsonify({\"error\": \"Session not found.\"}), 404\n        \n        pdf_info = session_doc.get(\"pdf_preview_info\", {}).get(contract_type)\n        if not pdf_info or not pdf_info.get(\"url\"):\n            logger.warning(f\"PDF preview URL for {contract_type} contract not available\")\n            return jsonify({\"error\": f\"PDF preview for {contract_type} contract not available. Generate preview first.\"}), 404\n        \n        cloudinary_pdf_url = pdf_info[\"url\"]\n        user_facing_filename = pdf_info.get(\"user_facing_filename\", f\"{contract_type}_preview_{session_id[:8]}.pdf\")\n        \n        # Import utilities\n        from app.utils.file_helpers import clean_filename\n        import urllib.parse\n        import requests\n        \n        # Proxy download from Cloudinary\n        logger.info(f\"Proxying PDF download from Cloudinary: {cloudinary_pdf_url}\")\n        r = requests.get(cloudinary_pdf_url, stream=True, timeout=120)\n        r.raise_for_status()\n        \n        safe_filename = clean_filename(user_facing_filename)\n        encoded_filename = urllib.parse.quote(safe_filename)\n        \n        logger.info(f\"PDF download successful for {contract_type} contract\")\n        return Response(\n            r.iter_content(chunk_size=8192),\n            content_type='application/pdf',\n            headers={\n                'Content-Disposition': f'attachment; filename=\"{safe_filename}\"; filename*=UTF-8\\'\\'{encoded_filename}',\n                'Content-Security-Policy': \"default-src 'self'\",\n                'X-Content-Type-Options': 'nosniff'\n            }\n        )\n        \n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error fetching PDF from Cloudinary: {http_err.response.status_code}\")\n        return jsonify({\"error\": f\"Cloudinary denied access to PDF (Status {http_err.response.status_code}).\"}), http_err.response.status_code if http_err.response.status_code >= 400 else 500\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error fetching PDF from Cloudinary for download: {e}\")\n        return jsonify({\"error\": \"Could not fetch PDF from cloud storage.\"}), 500\n    except Exception as e:\n        logger.error(f\"Unexpected error during PDF download proxy: {e}\")\n        return jsonify({\"error\": \"An unexpected error occurred during download.\"}), 500\n\n\n@analysis_bp.route('/feedback/expert', methods=['POST'])\ndef submit_expert_feedback():\n    \"\"\"Submit expert feedback.\"\"\"\n    logger.info(\"Processing expert feedback submission\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for expert feedback\")\n        return jsonify({\"error\": \"Database service is currently unavailable.\"}), 503\n    \n    if not request.is_json:\n        logger.warning(\"Non-JSON request received for expert feedback\")\n        return jsonify({\"error\": \"Content-Type must be application/json.\"}), 415\n    \n    data = request.get_json()\n    session_id = request.cookies.get(\"session_id\") or data.get(\"session_id\")\n    term_id = data.get(\"term_id\")\n    feedback_data = data.get(\"feedback_data\")\n    expert_user_id = data.get(\"expert_user_id\", \"default_expert_id\")\n    expert_username = data.get(\"expert_username\", \"Default Expert\")\n    \n    logger.info(f\"Submitting expert feedback for session: {session_id}, term: {term_id}\")\n    \n    if not all([session_id, term_id, feedback_data]):\n        logger.warning(\"Incomplete data for expert feedback\")\n        return jsonify({\"error\": \"البيانات المطلوبة غير مكتملة (session_id, term_id, feedback_data)\"}), 400\n    \n    try:\n        # Get original term\n        original_term_doc = terms_collection.find_one({\"session_id\": session_id, \"term_id\": term_id})\n        snapshot_ai_data = {}\n        original_term_text = \"\"\n        \n        if original_term_doc:\n            original_term_text = original_term_doc.get(\"term_text\", \"\")\n            snapshot_ai_data = {\n                \"original_ai_is_valid_sharia\": original_term_doc.get(\"is_valid_sharia\"),\n                \"original_ai_sharia_issue\": original_term_doc.get(\"sharia_issue\"),\n                \"original_ai_modified_term\": original_term_doc.get(\"modified_term\"),\n                \"original_ai_reference_number\": original_term_doc.get(\"reference_number\")\n            }\n        \n        # Create feedback document\n        feedback_doc = {\n            \"session_id\": session_id,\n            \"term_id\": term_id,\n            \"original_term_text_snapshot\": original_term_text,\n            \"expert_user_id\": expert_user_id,\n            \"expert_username\": expert_username,\n            \"feedback_timestamp\": datetime.datetime.now(),\n            \"ai_initial_analysis_assessment\": {\n                \"is_correct_compliance\": feedback_data.get(\"aiAnalysisApproved\"),\n            },\n            \"expert_verdict_is_valid_sharia\": feedback_data.get(\"expertIsValidSharia\"),\n            \"expert_comment_on_term\": feedback_data.get(\"expertComment\"),\n            \"expert_corrected_sharia_issue\": feedback_data.get(\"expertCorrectedShariaIssue\"),\n            \"expert_corrected_reference\": feedback_data.get(\"expertCorrectedReference\"),\n            \"expert_final_suggestion_for_term\": feedback_data.get(\"expertCorrectedSuggestion\"),\n            \"snapshot_ai_data\": snapshot_ai_data\n        }\n        \n        # Store in expert feedback collection (create if doesn't exist)\n        expert_feedback_collection = get_contracts_collection().database[\"expert_feedback\"]\n        expert_feedback_collection.insert_one(feedback_doc)\n        \n        logger.info(f\"Expert feedback saved successfully for session {session_id}, term {term_id}\")\n        return jsonify({\n            \"success\": True,\n            \"message\": f\"تم حفظ ملاحظات الخبير للبند: {term_id}\",\n            \"session_id\": session_id,\n            \"term_id\": term_id\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error saving expert feedback: {str(e)}\")\n        return jsonify({\"error\": f\"فشل حفظ ملاحظات الخبير: {str(e)}\"}), 500\n\n\n@analysis_bp.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return jsonify({\n        \"status\": \"healthy\",\n        \"service\": \"Shariaa Contract Analyzer\",\n        \"timestamp\": datetime.datetime.now().isoformat()\n    })","size_bytes":39312},"app/routes/generation.py":{"content":"\"\"\"\nGeneration Routes\n\nContract generation endpoints.\n\"\"\"\n\nimport os\nimport json\nimport datetime\nimport logging\nimport tempfile\nfrom flask import Blueprint, request, jsonify, Response\nfrom werkzeug.utils import secure_filename\n\n# Import services\nfrom app.services.database import get_contracts_collection, get_terms_collection\n\nlogger = logging.getLogger(__name__)\ngeneration_bp = Blueprint('generation', __name__)\n\n\n@generation_bp.route('/generate_from_brief', methods=['POST'])\ndef generate_from_brief():\n    \"\"\"Generate contract from brief.\"\"\"\n    logger.info(\"Generating contract from brief\")\n    \n    if not request.is_json:\n        return jsonify({\"error\": \"Content-Type must be application/json.\"}), 415\n    \n    data = request.get_json()\n    brief = data.get(\"brief\")\n    contract_type = data.get(\"contract_type\", \"general\")\n    jurisdiction = data.get(\"jurisdiction\", \"Egypt\")\n    \n    if not brief:\n        return jsonify({\"error\": \"Brief is required.\"}), 400\n    \n    try:\n        from app.services.ai_service import send_text_to_remote_api, get_chat_session\n        from config.default import DefaultConfig\n        \n        # Create generation prompt\n        generation_prompt = f\"\"\"\n        Generate a Sharia-compliant contract based on the following brief:\n        \n        Brief: {brief}\n        Contract Type: {contract_type}\n        Jurisdiction: {jurisdiction}\n        \n        Please provide a complete contract in Arabic that follows Islamic law principles.\n        \"\"\"\n        \n        # Send to AI service\n        response = send_text_to_remote_api(generation_prompt)\n        \n        if not response:\n            return jsonify({\"error\": \"Failed to generate contract.\"}), 500\n        \n        # Generate session ID\n        session_id = f\"gen_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        \n        # Save generation result\n        contracts_collection = get_contracts_collection()\n        if contracts_collection:\n            generation_doc = {\n                \"_id\": session_id,\n                \"generation_type\": \"from_brief\",\n                \"brief\": brief,\n                \"contract_type\": contract_type,\n                \"jurisdiction\": jurisdiction,\n                \"generated_contract\": response,\n                \"created_at\": datetime.datetime.now(),\n                \"status\": \"completed\"\n            }\n            contracts_collection.insert_one(generation_doc)\n        \n        return jsonify({\n            \"message\": \"Contract generated successfully.\",\n            \"session_id\": session_id,\n            \"generated_contract\": response,\n            \"contract_type\": contract_type,\n            \"jurisdiction\": jurisdiction\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error generating contract from brief: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error during generation.\"}), 500\n\n\n@generation_bp.route('/generate_modified_contract', methods=['POST'])\ndef generate_modified_contract():\n    \"\"\"Generate modified contract.\"\"\"\n    logger.info(\"Generating modified contract\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for contract generation\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    # Get session ID from cookie or request data\n    session_id = request.cookies.get(\"session_id\")\n    if request.is_json:\n        data = request.get_json()\n        session_id = session_id or data.get(\"session_id\")\n    \n    if not session_id:\n        logger.warning(\"No session ID provided for contract generation\")\n        return jsonify({\"error\": \"No session ID provided.\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for contract generation: {session_id}\")\n            return jsonify({\"error\": \"Session not found.\"}), 404\n        \n        original_filename = session_doc.get(\"original_filename\", \"contract.docx\")\n        contract_lang = session_doc.get(\"detected_contract_language\", \"ar\")\n        confirmed_terms = session_doc.get(\"confirmed_terms\", {})\n        \n        logger.info(f\"Contract language: {contract_lang}, Confirmed terms: {len(confirmed_terms)}\")\n        \n        # Get contract source\n        markdown_source = session_doc.get(\"generated_markdown_from_docx\") or session_doc.get(\"original_contract_markdown\")\n        if not markdown_source:\n            logger.error(\"Contract source text (markdown) not found for generation\")\n            return jsonify({\"error\": \"Contract source text not found for generation.\"}), 500\n        \n        # Import document processing services\n        from app.services.document_processor import create_docx_from_llm_markdown\n        from app.services.cloudinary_service import upload_to_cloudinary_helper\n        from app.utils.file_helpers import clean_filename\n        \n        # Create temporary file for modified contract\n        temp_dir = tempfile.gettempdir()\n        temp_docx_path = os.path.join(temp_dir, f\"modified_{session_id}.docx\")\n        \n        # Generate modified DOCX using confirmed terms\n        success = create_docx_from_llm_markdown(\n            markdown_source, \n            temp_docx_path, \n            confirmed_terms=confirmed_terms,\n            contract_language=contract_lang\n        )\n        \n        if not success:\n            return jsonify({\"error\": \"Failed to create modified contract document.\"}), 500\n        \n        # Upload to Cloudinary\n        cloudinary_folder = f\"shariaa_analyzer/{session_id}/modified_contracts\"\n        cloudinary_result = upload_to_cloudinary_helper(temp_docx_path, cloudinary_folder)\n        \n        # Update session with modified contract info\n        modified_contract_info = {\n            \"docx_cloudinary_info\": cloudinary_result,\n            \"generated_at\": datetime.datetime.now(),\n            \"confirmed_terms_count\": len(confirmed_terms)\n        }\n        \n        contracts_collection.update_one(\n            {\"_id\": session_id},\n            {\"$set\": {\"modified_contract_info\": modified_contract_info}}\n        )\n        \n        # Cleanup temp file\n        try:\n            os.remove(temp_docx_path)\n        except:\n            pass\n        \n        logger.info(f\"Modified contract generated successfully for session: {session_id}\")\n        return jsonify({\n            \"message\": \"Modified contract generated successfully.\",\n            \"session_id\": session_id,\n            \"download_url\": cloudinary_result.get(\"url\") if cloudinary_result else None,\n            \"confirmed_terms_count\": len(confirmed_terms)\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error generating modified contract: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error during contract generation.\"}), 500\n\n\n@generation_bp.route('/generate_marked_contract', methods=['POST'])\ndef generate_marked_contract():\n    \"\"\"Generate marked contract with highlighted terms.\"\"\"\n    logger.info(\"Generating marked contract\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for marked contract generation\")\n        return jsonify({\"error\": \"Database service unavailable.\"}), 503\n    \n    # Get session ID from cookie or request data\n    session_id = request.cookies.get(\"session_id\")\n    if request.is_json:\n        data = request.get_json()\n        session_id = session_id or data.get(\"session_id\")\n    \n    if not session_id:\n        logger.warning(\"No session ID provided for marked contract generation\")\n        return jsonify({\"error\": \"No session ID provided.\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for marked contract generation: {session_id}\")\n            return jsonify({\"error\": \"Session not found.\"}), 404\n        \n        original_filename = session_doc.get(\"original_filename\", \"contract.docx\")\n        contract_lang = session_doc.get(\"detected_contract_language\", \"ar\")\n        \n        # Get contract source\n        markdown_source = session_doc.get(\"generated_markdown_from_docx\") or session_doc.get(\"original_contract_markdown\")\n        if not markdown_source:\n            logger.error(\"Contract source text (markdown) not found for marked contract generation\")\n            return jsonify({\"error\": \"Contract source text not found for generation.\"}), 500\n        \n        # Get terms for marking\n        db_terms_list = list(terms_collection.find({\"session_id\": session_id}))\n        logger.info(f\"Found {len(db_terms_list)} terms for marking\")\n        \n        # Import document processing services\n        from app.services.document_processor import create_docx_from_llm_markdown\n        from app.services.cloudinary_service import upload_to_cloudinary_helper\n        from app.utils.file_helpers import clean_filename\n        \n        # Create temporary file for marked contract\n        temp_dir = tempfile.gettempdir()\n        temp_docx_path = os.path.join(temp_dir, f\"marked_{session_id}.docx\")\n        \n        # Generate marked DOCX with terms highlighting\n        success = create_docx_from_llm_markdown(\n            markdown_source, \n            temp_docx_path, \n            terms_for_marking=db_terms_list,\n            contract_language=contract_lang\n        )\n        \n        if not success:\n            return jsonify({\"error\": \"Failed to create marked contract document.\"}), 500\n        \n        # Upload to Cloudinary\n        cloudinary_folder = f\"shariaa_analyzer/{session_id}/marked_contracts\"\n        cloudinary_result = upload_to_cloudinary_helper(temp_docx_path, cloudinary_folder)\n        \n        # Update session with marked contract info\n        marked_contract_info = {\n            \"docx_cloudinary_info\": cloudinary_result,\n            \"generated_at\": datetime.datetime.now(),\n            \"marked_terms_count\": len(db_terms_list)\n        }\n        \n        contracts_collection.update_one(\n            {\"_id\": session_id},\n            {\"$set\": {\"marked_contract_info\": marked_contract_info}}\n        )\n        \n        # Cleanup temp file\n        try:\n            os.remove(temp_docx_path)\n        except:\n            pass\n        \n        logger.info(f\"Marked contract generated successfully for session: {session_id}\")\n        return jsonify({\n            \"message\": \"Marked contract generated successfully.\",\n            \"session_id\": session_id,\n            \"download_url\": cloudinary_result.get(\"url\") if cloudinary_result else None,\n            \"marked_terms_count\": len(db_terms_list)\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error generating marked contract: {str(e)}\")\n        return jsonify({\"error\": \"Internal server error during contract generation.\"}), 500","size_bytes":11020},"app/routes/interaction.py":{"content":"\"\"\"\nInteraction Routes\n\nUser interaction and consultation endpoints.\n\"\"\"\n\nimport json\nimport datetime\nimport logging\nfrom flask import Blueprint, request, jsonify\n\n# Import services\nfrom app.services.database import get_contracts_collection, get_terms_collection\n\nlogger = logging.getLogger(__name__)\ninteraction_bp = Blueprint('interaction', __name__)\n\n\n@interaction_bp.route('/interact', methods=['POST'])\ndef interact():\n    \"\"\"Interactive consultation.\"\"\"\n    logger.info(\"Processing interaction request\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for interaction\")\n        return jsonify({\"error\": \"Database service is currently unavailable.\"}), 503\n    \n    if not request.is_json:\n        logger.warning(\"Non-JSON request received for interaction\")\n        return jsonify({\"error\": \"Content-Type must be application/json.\"}), 415\n    \n    interaction_data = request.get_json()\n    if not interaction_data or \"question\" not in interaction_data:\n        logger.warning(\"Invalid interaction request - missing question\")\n        return jsonify({\"error\": \"الرجاء إرسال سؤال في صيغة JSON\"}), 400\n    \n    user_question = interaction_data.get(\"question\")\n    term_id_context = interaction_data.get(\"term_id\")\n    term_text_context = interaction_data.get(\"term_text\")\n    \n    session_id = request.cookies.get(\"session_id\") or request.args.get(\"session_id\") or interaction_data.get(\"session_id\")\n    \n    logger.info(f\"Processing interaction for session: {session_id}, term: {term_id_context or 'general'}\")\n    \n    if not session_id:\n        logger.warning(\"No session ID provided for interaction\")\n        return jsonify({\"error\": \"لم يتم العثور على جلسة. يرجى تحميل العقد أولاً.\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for interaction: {session_id}\")\n            return jsonify({\"error\": \"الجلسة غير موجودة أو منتهية الصلاحية\"}), 404\n        \n        contract_lang = session_doc.get(\"detected_contract_language\", \"ar\")\n        \n        # Import AI service\n        from app.services.ai_service import get_chat_session\n        from config.default import DefaultConfig\n        \n        # Get interaction prompt from config\n        config = DefaultConfig()\n        \n        # Get analysis type from session (already fetched above)\n        analysis_type = session_doc.get(\"analysis_type\", \"sharia\")\n            \n        # Select appropriate interaction prompt\n        if analysis_type == \"legal\":\n            interaction_prompt = getattr(config, 'INTERACTION_PROMPT_LEGAL', config.INTERACTION_PROMPT_SHARIA)\n        else:\n            interaction_prompt = config.INTERACTION_PROMPT_SHARIA\n        \n        try:\n            formatted_interaction_prompt = interaction_prompt.format(output_language=contract_lang)\n        except KeyError as ke:\n            logger.warning(f\"KeyError formatting INTERACTION_PROMPT: {ke}. Using default language 'ar'\")\n            formatted_interaction_prompt = interaction_prompt.format(output_language='ar')\n        \n        # Get contract context\n        full_contract_context = session_doc.get(\"original_contract_plain\", session_doc.get(\"original_contract_markdown\", \"\"))\n        \n        initial_analysis_summary_str = \"\"\n        if term_id_context:\n            term_doc_from_db = terms_collection.find_one({\"session_id\": session_id, \"term_id\": term_id_context})\n            if term_doc_from_db:\n                initial_analysis_summary_str = (\n                    f\"ملخص التحليل الأولي للبند '{term_id_context}' (لغة التحليل الأصلية: {contract_lang}):\\n\"\n                    f\"  - هل هو متوافق شرعاً؟ {'نعم' if term_doc_from_db.get('is_valid_sharia') else 'لا'}\\n\"\n                    f\"  - المشكلة الشرعية (إن وجدت): {term_doc_from_db.get('sharia_issue', 'لا يوجد')}\\n\"\n                    f\"  - النص المقترح للتعديل: {term_doc_from_db.get('modified_term', 'لا يوجد')}\\n\"\n                    f\"  - المرجع الشرعي: {term_doc_from_db.get('reference_number', 'لا يوجد')}\\n\"\n                )\n        \n        # Build full context for LLM\n        full_prompt_context = f\"\"\"\n        === سياق العقد ===\n        {full_contract_context[:2000]}  # Limit context size\n        \n        === تحليل البند المحدد ===\n        {initial_analysis_summary_str}\n        \n        === سؤال المستخدم ===\n        {user_question}\n        \"\"\"\n        \n        # Get chat session and send question\n        chat = get_chat_session(f\"{session_id}_interaction\", system_instruction=formatted_interaction_prompt)\n        response = chat.send_message(full_prompt_context)\n        \n        if not response or not response.text:\n            logger.error(\"Empty response from AI service\")\n            return jsonify({\"error\": \"لم نتمكن من الحصول على رد من الخدمة. حاول مرة أخرى.\"}), 500\n        \n        if response.text.startswith(\"ERROR_PROMPT_BLOCKED\") or response.text.startswith(\"ERROR_CONTENT_BLOCKED\"):\n            logger.warning(f\"Interaction blocked: {response.text}\")\n            return jsonify({\"error\": f\"محتوى محظور: {response.text}\"}), 400\n        \n        # Clean response\n        from app.utils.text_processing import clean_model_response\n        cleaned_response = clean_model_response(response.text)\n        \n        logger.info(f\"Interaction processed successfully for session: {session_id}\")\n        return jsonify({\n            \"answer\": cleaned_response,\n            \"session_id\": session_id,\n            \"term_id\": term_id_context,\n            \"contract_language\": contract_lang,\n            \"timestamp\": datetime.datetime.now().isoformat()\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error processing interaction: {str(e)}\")\n        return jsonify({\"error\": \"حدث خطأ أثناء معالجة السؤال. حاول مرة أخرى.\"}), 500\n\n\n@interaction_bp.route('/review_modification', methods=['POST'])\ndef review_modification():\n    \"\"\"Review user modifications.\"\"\"\n    logger.info(\"Processing review modification request\")\n    \n    contracts_collection = get_contracts_collection()\n    \n    if contracts_collection is None:\n        logger.error(\"Database service unavailable for review modification\")\n        return jsonify({\"error\": \"Database service is currently unavailable.\"}), 503\n    \n    if not request.is_json:\n        logger.warning(\"Non-JSON request received for review modification\")\n        return jsonify({\"error\": \"Content-Type must be application/json.\"}), 415\n    \n    data = request.get_json()\n    session_id = request.cookies.get(\"session_id\") or data.get(\"session_id\")\n    term_id = data.get(\"term_id\")\n    user_modified_text = data.get(\"user_modified_text\")\n    original_term_text = data.get(\"original_term_text\")\n    \n    logger.info(f\"Reviewing modification for session: {session_id}, term: {term_id}\")\n    \n    if not all([session_id, term_id, user_modified_text is not None, original_term_text is not None]):\n        logger.warning(\"Incomplete data for review modification\")\n        return jsonify({\"error\": \"بيانات ناقصة\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for review modification: {session_id}\")\n            return jsonify({\"error\": \"الجلسة غير موجودة\"}), 404\n        \n        contract_lang = session_doc.get(\"detected_contract_language\", \"ar\")\n        \n        # Import AI service\n        from app.services.ai_service import get_chat_session\n        from config.default import DefaultConfig\n        \n        # Get review prompt from config\n        config = DefaultConfig()\n        \n        # Get analysis type from session (already fetched above)\n        analysis_type = session_doc.get(\"analysis_type\", \"sharia\")\n            \n        # Select appropriate review prompt\n        if analysis_type == \"legal\":\n            review_prompt = getattr(config, 'REVIEW_MODIFICATION_PROMPT_LEGAL', '')\n        else:\n            review_prompt = getattr(config, 'REVIEW_MODIFICATION_PROMPT_SHARIA', '')\n        \n        try:\n            formatted_review_prompt = review_prompt.format(output_language=contract_lang)\n        except KeyError as ke:\n            logger.error(f\"KeyError in REVIEW_MODIFICATION_PROMPT: {ke}\")\n            return jsonify({\"error\": f\"Prompt format error: {ke}\"}), 500\n        \n        # Create review payload\n        review_payload = json.dumps({\n            \"original_term_text\": original_term_text,\n            \"user_modified_text\": user_modified_text\n        }, ensure_ascii=False, indent=2)\n        \n        # Send to AI service\n        logger.info(\"Sending modification review to AI service\")\n        chat = get_chat_session(f\"{session_id}_review_{term_id}\", system_instruction=formatted_review_prompt, force_new=True)\n        response = chat.send_message(review_payload)\n        \n        if not response or not response.text:\n            logger.error(\"Empty response from AI service for review\")\n            return jsonify({\"error\": \"لم نتمكن من الحصول على رد من الخدمة. حاول مرة أخرى.\"}), 500\n        \n        if response.text.startswith(\"ERROR_PROMPT_BLOCKED\") or response.text.startswith(\"ERROR_CONTENT_BLOCKED\"):\n            logger.warning(f\"Review modification blocked: {response.text}\")\n            return jsonify({\"error\": f\"محتوى محظور: {response.text}\"}), 400\n        \n        # Clean response\n        from app.utils.text_processing import clean_model_response\n        cleaned_response = clean_model_response(response.text)\n        \n        logger.info(f\"Modification review completed for session: {session_id}, term: {term_id}\")\n        return jsonify({\n            \"review_result\": cleaned_response,\n            \"session_id\": session_id,\n            \"term_id\": term_id,\n            \"contract_language\": contract_lang,\n            \"timestamp\": datetime.datetime.now().isoformat()\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error reviewing modification: {str(e)}\")\n        return jsonify({\"error\": \"حدث خطأ أثناء مراجعة التعديل. حاول مرة أخرى.\"}), 500\n\n\n@interaction_bp.route('/confirm_modification', methods=['POST'])\ndef confirm_modification():\n    \"\"\"Confirm user modifications.\"\"\"\n    logger.info(\"Processing confirm modification request\")\n    \n    contracts_collection = get_contracts_collection()\n    terms_collection = get_terms_collection()\n    \n    if contracts_collection is None or terms_collection is None:\n        logger.error(\"Database service unavailable for confirm modification\")\n        return jsonify({\"error\": \"Database service is currently unavailable.\"}), 503\n    \n    data = request.get_json()\n    if not data:\n        logger.warning(\"No data sent in confirm modification request\")\n        return jsonify({\"error\": \"لم يتم إرسال بيانات في الطلب\"}), 400\n    \n    term_id = data.get(\"term_id\")\n    modified_text = data.get(\"modified_text\")\n    session_id = request.cookies.get(\"session_id\") or data.get(\"session_id\")\n    \n    logger.info(f\"Confirming modification for session: {session_id}, term: {term_id}\")\n    \n    if term_id is None or modified_text is None or not session_id:\n        logger.warning(\"Incomplete data for confirm modification\")\n        return jsonify({\"error\": \"البيانات المطلوبة غير مكتملة\"}), 400\n    \n    try:\n        session_doc = contracts_collection.find_one({\"_id\": session_id})\n        if not session_doc:\n            logger.warning(f\"Session not found for confirm modification: {session_id}\")\n            return jsonify({\"error\": \"الجلسة غير موجودة\"}), 404\n        \n        # Update confirmed terms in session\n        updated_confirmed_terms = session_doc.get(\"confirmed_terms\", {})\n        \n        # Get original term text\n        term_doc = terms_collection.find_one({\"session_id\": session_id, \"term_id\": term_id})\n        if term_doc:\n            updated_confirmed_terms[str(term_id)] = {\n                \"original_text\": term_doc.get(\"term_text\", \"\"),\n                \"confirmed_text\": modified_text\n            }\n        else:\n            logger.warning(f\"Original term not found in DB for confirmation: {term_id}\")\n            updated_confirmed_terms[str(term_id)] = {\n                \"original_text\": \"\",\n                \"confirmed_text\": modified_text\n            }\n        \n        # Update database\n        contracts_collection.update_one(\n            {\"_id\": session_id},\n            {\"$set\": {\"confirmed_terms\": updated_confirmed_terms}}\n        )\n        \n        terms_collection.update_one(\n            {\"session_id\": session_id, \"term_id\": term_id},\n            {\"$set\": {\n                \"is_confirmed_by_user\": True,\n                \"confirmed_modified_text\": modified_text,\n            }}\n        )\n        \n        logger.info(f\"Modification confirmed for session {session_id}, term {term_id}\")\n        return jsonify({\n            \"success\": True, \n            \"message\": f\"تم تأكيد التعديل للبند: {term_id}\",\n            \"session_id\": session_id,\n            \"term_id\": term_id\n        })\n        \n    except Exception as e:\n        logger.error(f\"Error confirming modification: {str(e)}\")\n        return jsonify({\"error\": f\"خطأ أثناء تأكيد التعديل: {str(e)}\"}), 500","size_bytes":13837},"app/services/__init__.py":{"content":"# Services package","size_bytes":18},"app/services/ai_service.py":{"content":"\"\"\"\nAI Service for Google Generative AI integration.\nConsolidated from original remote_api.py\n\"\"\"\n\nimport pathlib\nimport google.generativeai as genai\nimport time\nimport traceback\nimport json\nimport logging\nfrom flask import current_app\n\nlogger = logging.getLogger(__name__)\n\n# Global chat sessions storage\nchat_sessions = {}\n\ndef init_ai_service(app):\n    \"\"\"Initialize AI service with configuration.\"\"\"\n    try:\n        google_api_key = app.config.get('GOOGLE_API_KEY')\n        if not google_api_key:\n            logger.warning(\"GOOGLE_API_KEY not configured - AI services will be unavailable\")\n            return\n            \n        genai.configure(api_key=google_api_key)\n        logger.info(\"Google Generative AI configured successfully\")\n    except Exception as e:\n        logger.error(f\"Error configuring Google Generative AI: {e}\")\n        traceback.print_exc()\n\ndef get_chat_session(session_id_key: str, system_instruction: str | None = None, force_new: bool = False):\n    \"\"\"Get or create a chat session for AI interactions.\"\"\"\n    global chat_sessions\n    session_id_key = session_id_key or \"default_chat_session_key\"\n\n    if force_new or session_id_key not in chat_sessions:\n        if force_new and session_id_key in chat_sessions:\n            logger.info(f\"Forcing new chat session for key (was existing): {session_id_key}\")\n        else:\n            logger.info(f\"Creating new chat session for key: {session_id_key}\")\n        try:\n            model_name = current_app.config.get('MODEL_NAME', 'gemini-2.5-flash')\n            temperature = current_app.config.get('TEMPERATURE', 0)\n            \n            generation_config = genai.GenerationConfig(temperature=temperature)\n            safety_settings = [\n                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n            ]\n            model = genai.GenerativeModel(\n                model_name,\n                generation_config=generation_config,\n                system_instruction=system_instruction, \n                safety_settings=safety_settings\n            )\n            chat_sessions[session_id_key] = model.start_chat(history=[]) \n        except Exception as e:\n            logger.error(f\"Failed to create GenerativeModel or start chat for session {session_id_key}: {e}\")\n            traceback.print_exc()\n            raise Exception(f\"فشل في بدء جلسة الدردشة مع النموذج: {e}\")\n    return chat_sessions[session_id_key]\n\ndef send_text_to_remote_api(text_payload: str, session_id_key: str, formatted_system_prompt: str):\n    \"\"\"Send text to AI API for processing.\"\"\"\n    if not text_payload or not text_payload.strip():\n        logger.warning(f\"Empty text_payload for session_id_key {session_id_key}\")\n        return \"\"\n\n    logger.info(f\"Sending text to LLM for session: {session_id_key}, payload length: {len(text_payload)}\")\n    \n    try:\n        chat = get_chat_session(session_id_key, system_instruction=formatted_system_prompt, force_new=True)\n        \n        max_retries = 3\n        for attempt in range(max_retries):\n            try:\n                logger.info(f\"Sending request to AI API (attempt {attempt + 1}/{max_retries})\")\n                response = chat.send_message(text_payload)\n                \n                if response and response.text:\n                    logger.info(f\"Received response from AI API: {len(response.text)} characters\")\n                    return response.text\n                else:\n                    logger.warning(f\"Empty response from AI API on attempt {attempt + 1}\")\n                    \n            except Exception as e:\n                logger.error(f\"AI API request failed on attempt {attempt + 1}: {e}\")\n                if attempt < max_retries - 1:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                else:\n                    raise\n        \n        return \"ERROR_API_FAILED\"\n        \n    except Exception as e:\n        logger.error(f\"Critical error in AI API communication: {e}\")\n        traceback.print_exc()\n        return f\"ERROR_API_COMMUNICATION: {str(e)}\"\n\ndef extract_text_from_file(file_path: str):\n    \"\"\"Extract text from PDF/TXT files using AI.\"\"\"\n    try:\n        logger.info(f\"Extracting text from file: {file_path}\")\n        \n        extraction_prompt = current_app.config.get('EXTRACTION_PROMPT', \n            \"Extract the full text from the provided file with high accuracy. Use **Markdown** format to preserve structure.\")\n            \n        # Upload file to AI service\n        sample_file = genai.upload_file(path=file_path, display_name=\"Contract Document\")\n        logger.info(f\"File uploaded to AI service: {sample_file.name}\")\n        \n        # Send extraction request\n        model_name = current_app.config.get('MODEL_NAME', 'gemini-2.5-flash')\n        model = genai.GenerativeModel(model_name)\n        response = model.generate_content([sample_file, extraction_prompt])\n        \n        if response and response.text:\n            logger.info(f\"Text extraction completed: {len(response.text)} characters\")\n            return response.text\n        else:\n            logger.error(\"No text extracted from file\")\n            return None\n            \n    except Exception as e:\n        logger.error(f\"Text extraction failed for {file_path}: {e}\")\n        traceback.print_exc()\n        return None","size_bytes":5604},"app/services/cloudinary_service.py":{"content":"\"\"\"\nCloudinary Service\n\nCloud storage management for the Shariaa Contract Analyzer.\n\"\"\"\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Import cloudinary with graceful fallback\ntry:\n    import cloudinary\n    import cloudinary.uploader\n    import cloudinary.api\n    CLOUDINARY_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"Cloudinary package not available. File upload features will be limited.\")\n    CLOUDINARY_AVAILABLE = False\n\n\ndef init_cloudinary(app):\n    \"\"\"Initialize Cloudinary configuration.\"\"\"\n    if not CLOUDINARY_AVAILABLE:\n        logger.warning(\"Cloudinary package not installed - file storage services will be unavailable\")\n        return\n    \n    try:\n        cloud_name = app.config.get('CLOUDINARY_CLOUD_NAME')\n        api_key = app.config.get('CLOUDINARY_API_KEY')\n        api_secret = app.config.get('CLOUDINARY_API_SECRET')\n        \n        if not all([cloud_name, api_key, api_secret]):\n            logger.warning(\"Cloudinary credentials not fully configured - file storage services will be limited\")\n            return\n            \n        cloudinary.config(\n            cloud_name=cloud_name,\n            api_key=api_key,\n            api_secret=api_secret,\n            secure=True\n        )\n        logger.info(\"Cloudinary configured successfully\")\n    except Exception as e:\n        logger.error(f\"Cloudinary configuration failed: {e}\")\n        logger.warning(\"Cloudinary services will be unavailable\")\n\n\ndef upload_to_cloudinary_helper(\n    local_file_path: str,\n    cloudinary_folder: str,\n    resource_type: str = \"auto\",\n    public_id_prefix: str = \"\",\n    custom_public_id: str = None\n):\n    \"\"\"Upload a file to Cloudinary with helper options.\"\"\"\n    if not CLOUDINARY_AVAILABLE:\n        logger.error(\"Cloudinary not available for upload\")\n        return None\n        \n    try:\n        import uuid\n        import traceback\n        \n        upload_options = {\n            \"folder\": cloudinary_folder,\n            \"resource_type\": resource_type,\n            \"overwrite\": True\n        }\n        \n        if custom_public_id:\n            upload_options[\"public_id\"] = custom_public_id\n        elif public_id_prefix:\n            upload_options[\"public_id\"] = f\"{public_id_prefix}_{uuid.uuid4().hex[:8]}\"\n            \n        result = cloudinary.uploader.upload(local_file_path, **upload_options)\n        \n        if result and result.get(\"secure_url\"):\n            logger.info(f\"File uploaded to Cloudinary: {result['secure_url']}\")\n            return result\n        else:\n            logger.error(\"Cloudinary upload failed - no secure URL returned\")\n            return None\n            \n    except Exception as e:\n        logger.error(f\"Error uploading to Cloudinary: {e}\")\n        traceback.print_exc()\n        return None","size_bytes":2770},"app/services/database.py":{"content":"\"\"\"\nDatabase Service\n\nMongoDB connection and management for the Shariaa Contract Analyzer.\n\"\"\"\n\nimport logging\nfrom pymongo import MongoClient\nfrom flask import current_app\n\nlogger = logging.getLogger(__name__)\n\n# Global database connections\nclient = None\ndb = None\ncontracts_collection = None\nterms_collection = None\nexpert_feedback_collection = None\n\nDB_NAME = \"shariaa_analyzer_db\"\n\n\ndef init_db(app):\n    \"\"\"Initialize database connection.\"\"\"\n    global client, db, contracts_collection, terms_collection, expert_feedback_collection\n    \n    try:\n        mongo_uri = app.config.get('MONGO_URI')\n        if not mongo_uri:\n            logger.warning(\"MONGO_URI not configured - database services will be unavailable\")\n            return\n            \n        logger.info(\"Attempting to connect to MongoDB...\")\n        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=45000)\n        client.admin.command('ping')\n        db = client[DB_NAME]\n        contracts_collection = db.contracts\n        terms_collection = db.terms\n        expert_feedback_collection = db.expert_feedback\n        logger.info(f\"Successfully connected to MongoDB: {DB_NAME}\")\n    except Exception as e:\n        logger.error(f\"MongoDB connection failed: {e}\")\n        logger.warning(\"Database services will be unavailable\")\n        # Set collections to None so endpoints can handle gracefully\n        client = None\n        db = None\n        contracts_collection = None\n        terms_collection = None\n        expert_feedback_collection = None\n\n\ndef get_contracts_collection():\n    \"\"\"Get contracts collection.\"\"\"\n    return contracts_collection\n\n\ndef get_terms_collection():\n    \"\"\"Get terms collection.\"\"\"\n    return terms_collection\n\n\ndef get_expert_feedback_collection():\n    \"\"\"Get expert feedback collection.\"\"\"\n    return expert_feedback_collection","size_bytes":1831},"app/services/document_processor.py":{"content":"\"\"\"\nDocument processing service for DOCX manipulation and conversion.\nConsolidated from original doc_processing.py\n\"\"\"\n\nimport os\nimport uuid\nimport re\nimport traceback\nimport subprocess\nimport tempfile\nfrom docx import Document as DocxDocument\nfrom docx.shared import Pt, RGBColor, Inches, Cm\nfrom docx.enum.text import WD_PARAGRAPH_ALIGNMENT, WD_LINE_SPACING, WD_BREAK \nfrom docx.enum.style import WD_STYLE_TYPE\nfrom docx.enum.table import WD_TABLE_DIRECTION, WD_TABLE_ALIGNMENT\nfrom docx.oxml import OxmlElement\nfrom docx.oxml.ns import qn\nfrom docx.oxml.text.paragraph import CT_P\nfrom docx.oxml.table import CT_Tbl, CT_TcPr\nfrom docx.text.paragraph import Paragraph\nfrom docx.table import Table, _Cell\nimport logging\nfrom flask import current_app\n\nfrom app.utils.file_helpers import ensure_dir, clean_filename\nfrom app.utils.text_processing import clean_model_response\n\nlogger = logging.getLogger(__name__)\n\n\ndef extract_text_from_file(file_path):\n    \"\"\"\n    Extract text from a file (placeholder implementation).\n    \n    Args:\n        file_path (str): Path to the file to extract text from\n        \n    Returns:\n        str: Extracted text content\n    \"\"\"\n    try:\n        # Simple implementation - for DOCX files\n        if file_path.lower().endswith('.docx'):\n            doc = DocxDocument(file_path)\n            paragraphs = []\n            for paragraph in doc.paragraphs:\n                if paragraph.text.strip():\n                    paragraphs.append(paragraph.text)\n            return '\\n'.join(paragraphs)\n        else:\n            # For other file types, return placeholder\n            return \"Text extraction not implemented for this file type\"\n    except Exception as e:\n        logger.error(f\"Error extracting text from {file_path}: {str(e)}\")\n        return f\"Error extracting text: {str(e)}\"\n\ndef build_structured_text_for_analysis(doc: DocxDocument) -> tuple[str, str]:\n    \"\"\"\n    Extracts text from a DOCX document, converting it to a markdown-like format\n    that preserves bold, italic, and underline formatting, while also assigning\n    unique IDs to paragraphs and table cell content for precise term identification.\n    Returns a structured markdown string with IDs and a plain text version.\n    \"\"\"\n    structured_markdown = []\n    plain_text_parts = []\n    para_idx_counter_body = 0\n    table_idx_counter_body = 0\n\n    for element in doc.element.body:\n        if isinstance(element, CT_P):\n            para = Paragraph(element, doc)\n            if para.text.strip():\n                para_id = f\"para_{para_idx_counter_body}\"\n                \n                # Convert paragraph to markdown while preserving formatting\n                markdown_line = \"\"\n                for run in para.runs:\n                    text = run.text\n                    if run.bold: text = f\"**{text}**\"\n                    if run.italic: text = f\"*{text}*\"\n                    if run.underline: text = f\"__{text}__\"\n                    markdown_line += text\n                \n                structured_markdown.append(f\"[[ID:{para_id}]]\\n{markdown_line}\")\n                plain_text_parts.append(para.text)\n                para_idx_counter_body += 1\n\n        elif isinstance(element, CT_Tbl):\n            table = Table(element, doc)\n            table_id_prefix = f\"table_{table_idx_counter_body}\"\n            structured_markdown.append(f\"[[TABLE_START:{table_id_prefix}]]\")\n            plain_text_parts.append(f\"[جدول {table_idx_counter_body+1}]\")\n\n            # Convert table to markdown table format\n            md_table = []\n            for r_idx, row in enumerate(table.rows):\n                row_text_parts = []\n                row_plain_parts = []\n                for c_idx, cell in enumerate(row.cells):\n                    cell_id_prefix = f\"{table_id_prefix}_r{r_idx}_c{c_idx}\"\n                    cell_para_idx_counter = 0\n                    cell_markdown_content = \"\"\n                    cell_plain_content = \"\"\n\n                    for para_in_cell in cell.paragraphs:\n                        if para_in_cell.text.strip():\n                            cell_para_id = f\"{cell_id_prefix}_p{cell_para_idx_counter}\"\n                            \n                            # Convert cell paragraph to markdown\n                            cell_markdown_line = \"\"\n                            for run in para_in_cell.runs:\n                                text = run.text\n                                if run.bold: text = f\"**{text}**\"\n                                if run.italic: text = f\"*{text}*\"\n                                if run.underline: text = f\"__{text}__\"\n                                cell_markdown_line += text\n                            \n                            cell_markdown_content += f\"[[ID:{cell_para_id}]] {cell_markdown_line} \"\n                            cell_plain_content += para_in_cell.text + \" \"\n                            cell_para_idx_counter += 1\n\n                    row_text_parts.append(cell_markdown_content.strip())\n                    row_plain_parts.append(cell_plain_content.strip())\n\n                md_table.append(\"| \" + \" | \".join(row_text_parts) + \" |\")\n                plain_text_parts.extend(row_plain_parts)\n\n            structured_markdown.extend(md_table)\n            structured_markdown.append(f\"[[TABLE_END:{table_id_prefix}]]\")\n            table_idx_counter_body += 1\n\n    return \"\\n\".join(structured_markdown), \"\\n\".join(plain_text_parts)\n\ndef convert_docx_to_pdf(docx_file_path: str, output_folder: str) -> str | None:\n    \"\"\"Convert a DOCX file to PDF using LibreOffice.\"\"\"\n    try:\n        ensure_dir(output_folder)\n        \n        libreoffice_path = current_app.config.get('LIBREOFFICE_PATH', 'libreoffice')\n        \n        logger.info(f\"Converting DOCX to PDF: {docx_file_path} -> {output_folder}\")\n        \n        # Run LibreOffice headless conversion\n        command = [\n            libreoffice_path,\n            \"--headless\",\n            \"--convert-to\", \"pdf\",\n            \"--outdir\", output_folder,\n            docx_file_path\n        ]\n        \n        result = subprocess.run(command, capture_output=True, text=True, timeout=60)\n        \n        if result.returncode == 0:\n            # Find the generated PDF file\n            docx_basename = os.path.splitext(os.path.basename(docx_file_path))[0]\n            pdf_path = os.path.join(output_folder, f\"{docx_basename}.pdf\")\n            \n            if os.path.exists(pdf_path):\n                logger.info(f\"PDF conversion successful: {pdf_path}\")\n                return pdf_path\n            else:\n                logger.error(f\"PDF file not found after conversion: {pdf_path}\")\n                return None\n        else:\n            logger.error(f\"LibreOffice conversion failed: {result.stderr}\")\n            return None\n            \n    except subprocess.TimeoutExpired:\n        logger.error(\"LibreOffice conversion timed out\")\n        return None\n    except Exception as e:\n        logger.error(f\"Error during PDF conversion: {e}\")\n        traceback.print_exc()\n        return None\n\ndef create_docx_from_llm_markdown(\n    original_markdown_text: str,\n    output_path: str,\n    contract_language: str = 'ar',\n    terms_for_marking: list[dict] | dict | None = None\n):\n    \"\"\"\n    Creates a professional DOCX document from markdown text with term highlighting.\n    Supports Arabic RTL layout and color coding for terms.\n    \"\"\"\n    try:\n        logger.info(f\"Creating DOCX from markdown: {output_path}\")\n        \n        # Create new document\n        doc = DocxDocument()\n        \n        # Set up styles for Arabic/RTL support\n        if contract_language == 'ar':\n            _setup_arabic_styles(doc)\n        \n        # Process the markdown text and create document content\n        _process_markdown_to_docx(doc, original_markdown_text, terms_for_marking, contract_language)\n        \n        # Save the document\n        ensure_dir(os.path.dirname(output_path))\n        doc.save(output_path)\n        \n        logger.info(f\"DOCX document created successfully: {output_path}\")\n        return output_path\n        \n    except Exception as e:\n        logger.error(f\"Error creating DOCX document: {e}\")\n        traceback.print_exc()\n        return None\n\ndef _setup_arabic_styles(doc):\n    \"\"\"Set up Arabic RTL styles for the document.\"\"\"\n    # Add Arabic font style\n    styles = doc.styles\n    \n    # Create Arabic paragraph style\n    try:\n        arabic_style = styles.add_style('Arabic Text', WD_STYLE_TYPE.PARAGRAPH)\n        arabic_style.font.name = 'Arabic Typesetting'\n        arabic_style.font.size = Pt(12)\n        arabic_style.paragraph_format.alignment = WD_PARAGRAPH_ALIGNMENT.RIGHT\n    except:\n        pass  # Style may already exist\n\ndef _process_markdown_to_docx(doc, markdown_text: str, terms_for_marking, contract_language: str):\n    \"\"\"Process markdown text and add to DOCX document with formatting.\"\"\"\n    lines = markdown_text.split('\\n')\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        # Handle ID markers\n        if line.startswith('[[ID:') and line.endswith(']]'):\n            continue  # Skip ID-only lines\n            \n        # Handle table markers\n        if line.startswith('[[TABLE_START:') or line.startswith('[[TABLE_END:'):\n            continue  # Skip table markers for now\n            \n        # Process regular text lines\n        para = doc.add_paragraph()\n        \n        if contract_language == 'ar':\n            para.alignment = WD_PARAGRAPH_ALIGNMENT.RIGHT\n            \n        # Add text with basic formatting\n        _add_formatted_text_to_paragraph(para, line, terms_for_marking)\n\ndef _add_formatted_text_to_paragraph(para, text: str, terms_for_marking):\n    \"\"\"Add formatted text to a paragraph, handling markdown formatting.\"\"\"\n    # Simple markdown processing for bold, italic, underline\n    # This is a simplified version - could be expanded for full markdown support\n    \n    # Remove ID markers if present\n    text = re.sub(r'\\[\\[ID:[^\\]]+\\]\\]\\s*', '', text)\n    \n    run = para.add_run(text)\n    \n    # Apply highlighting if this text matches terms for marking\n    if terms_for_marking:\n        _apply_term_highlighting(run, text, terms_for_marking)\n\ndef _apply_term_highlighting(run, text: str, terms_for_marking):\n    \"\"\"Apply highlighting to terms that need to be marked.\"\"\"\n    if not terms_for_marking:\n        return\n        \n    # Convert terms_for_marking to a list if it's a dict\n    if isinstance(terms_for_marking, dict):\n        terms_list = list(terms_for_marking.values())\n    else:\n        terms_list = terms_for_marking\n        \n    # Simple highlighting for terms\n    for term in terms_list:\n        if isinstance(term, dict) and 'term_text' in term:\n            if term['term_text'].strip() in text:\n                # Highlight non-compliant terms in red\n                if not term.get('is_valid_sharia', True):\n                    run.font.color.rgb = RGBColor(255, 0, 0)  # Red\n                else:\n                    run.font.color.rgb = RGBColor(0, 128, 0)  # Green","size_bytes":11051},"app/utils/__init__.py":{"content":"\"\"\"\nUtility modules for the Shariaa Contract Analyzer.\n\"\"\"","size_bytes":58},"app/utils/file_helpers.py":{"content":"\"\"\"\nFile handling utilities for the Shariaa Contract Analyzer.\nConsolidated from original utils.py and api_server.py\n\"\"\"\n\nimport os\nimport uuid\nimport re\nimport traceback\nimport tempfile\nimport requests\nfrom unidecode import unidecode\n\ndef ensure_dir(dir_path: str):\n    \"\"\"Ensures that a directory exists, creating it if necessary.\"\"\"\n    try:\n        os.makedirs(dir_path, exist_ok=True)\n    except OSError as e:\n        print(f\"ERROR: Could not create directory '{dir_path}': {e}\")\n        traceback.print_exc()\n        raise\n\ndef clean_filename(filename: str) -> str:\n    \"\"\"\n    Cleans a filename by removing potentially problematic characters and\n    ensuring it's a valid name for most filesystems.\n    Uses unidecode for broader character support before basic sanitization.\n    \"\"\"\n    if not filename:\n        return f\"contract_{uuid.uuid4().hex[:8]}\"\n\n    # Transliterate Unicode characters to ASCII equivalents\n    ascii_filename = unidecode(filename)\n    \n    # Replace spaces with underscores\n    safe_filename = ascii_filename.replace(\" \", \"_\")\n    \n    # Remove any character that is not a word character, whitespace (though spaces are gone), a hyphen, or a period.\n    safe_filename = re.sub(r'[^\\w\\s.-]', '', safe_filename).strip()\n\n    # If the cleaning results in an empty filename, generate a unique one.\n    if not safe_filename:\n        return f\"contract_{uuid.uuid4().hex[:8]}\"\n\n    # Truncate to a maximum length to avoid issues with filesystem limits.\n    # Ensure extension is preserved if possible.\n    max_len = 200 \n    if len(safe_filename) > max_len:\n        name, ext = os.path.splitext(safe_filename)\n        # Truncate the name part, then append extension\n        safe_filename = name[:max_len - len(ext) -1] + ext # -1 for the dot\n    return safe_filename\n\ndef download_file_from_url(file_url: str, suggested_filename: str, destination_folder: str) -> str | None:\n    \"\"\"Downloads a file from URL to local destination folder. Returns local file path or None.\"\"\"\n    try:\n        response = requests.get(file_url, stream=True, timeout=30)\n        response.raise_for_status()\n        \n        ensure_dir(destination_folder)\n        local_file_path = os.path.join(destination_folder, clean_filename(suggested_filename))\n        \n        with open(local_file_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        \n        return local_file_path\n    except Exception as e:\n        print(f\"Error downloading file from {file_url}: {e}\")\n        return None","size_bytes":2552},"app/utils/text_processing.py":{"content":"\"\"\"\nText processing utilities for the Shariaa Contract Analyzer.\nConsolidated from original utils.py and api_server.py\n\"\"\"\n\nimport re\nimport uuid\nimport logging\nfrom unidecode import unidecode\n\nlogger = logging.getLogger(__name__)\n\ndef clean_model_response(response_text: str | None) -> str:\n    \"\"\"\n    Cleans the response text from the model, attempting to extract JSON\n    content if it's wrapped in markdown code blocks or found directly.\n    For contract text, removes unwanted analysis and commentary.\n    \"\"\"\n    if not isinstance(response_text, str):\n        return \"\"\n\n    # Try to find JSON within ```json ... ```\n    json_match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", response_text, re.DOTALL | re.IGNORECASE)\n    if json_match:\n        return json_match.group(1).strip()\n\n    # Try to find JSON within ``` ... ``` (generic code block)\n    code_match = re.search(r\"```\\s*([\\s\\S]*?)\\s*```\", response_text, re.DOTALL)\n    if code_match:\n        content = code_match.group(1).strip()\n        # Check if the content looks like a JSON object or array\n        if (content.startswith('{') and content.endswith('}')) or \\\n           (content.startswith('[') and content.endswith(']')):\n            return content\n\n    # If no markdown blocks, try to find the first occurrence of '{' or '['\n    # and extract up to the matching '}' or ']'\n    if '{' in response_text:\n        start_idx = response_text.index('{')\n        bracket_count = 0\n        for i, char in enumerate(response_text[start_idx:], start_idx):\n            if char == '{':\n                bracket_count += 1\n            elif char == '}':\n                bracket_count -= 1\n                if bracket_count == 0:\n                    return response_text[start_idx:i+1]\n    \n    if '[' in response_text:\n        start_idx = response_text.index('[')\n        bracket_count = 0\n        for i, char in enumerate(response_text[start_idx:], start_idx):\n            if char == '[':\n                bracket_count += 1\n            elif char == ']':\n                bracket_count -= 1\n                if bracket_count == 0:\n                    return response_text[start_idx:i+1]\n\n    # If nothing else works, return the original text\n    return response_text.strip()\n\ndef translate_arabic_to_english(arabic_text):\n    \"\"\"\n    Translates Arabic contract names to English using simple transliteration.\n    Falls back to generic name if translation fails.\n    \"\"\"\n    try:\n        # Simple transliteration mapping for common Arabic words in contracts\n        transliteration_map = {\n            'عقد': 'contract',\n            'بيع': 'sale',\n            'شراء': 'purchase',\n            'إيجار': 'rental',\n            'تأجير': 'lease',\n            'عمل': 'work',\n            'خدمات': 'services',\n            'توريد': 'supply',\n            'مقاولة': 'contracting',\n            'شركة': 'company',\n            'مؤسسة': 'institution',\n            'الأول': 'first',\n            'الثاني': 'second',\n            'نهائي': 'final',\n            'مبدئي': 'preliminary'\n        }\n\n        # Clean and split the Arabic text\n        words = arabic_text.strip().split()\n        translated_words = []\n\n        for word in words:\n            # Remove common Arabic articles and prepositions\n            clean_word = word.replace('ال', '').replace('و', '').replace('في', '').replace('من', '')\n\n            # Look for direct translation\n            translated = transliteration_map.get(clean_word.lower())\n            if translated:\n                translated_words.append(translated)\n            else:\n                # Fallback: use unidecode for transliteration\n                transliterated = unidecode(clean_word)\n                if transliterated and transliterated.strip():\n                    translated_words.append(transliterated.lower())\n\n        if translated_words:\n            result = '_'.join(translated_words)[:50]  # Limit length\n            logger.info(f\"Translated Arabic contract name '{arabic_text}' to '{result}'\")\n            return result\n        else:\n            fallback = f\"contract_{uuid.uuid4().hex[:8]}\"\n            logger.warning(f\"Could not translate Arabic name '{arabic_text}', using fallback: {fallback}\")\n            return fallback\n\n    except Exception as e:\n        logger.error(f\"Error translating Arabic contract name '{arabic_text}': {e}\")\n        return f\"contract_{uuid.uuid4().hex[:8]}\"\n\ndef generate_safe_public_id(base_name, prefix=\"\", max_length=50):\n    \"\"\"\n    Generates a safe, short public_id for Cloudinary uploads.\n    Handles Arabic names by translating them to English.\n    \"\"\"\n    try:\n        if not base_name:\n            safe_id = f\"{prefix}_{uuid.uuid4().hex[:8]}\"\n            logger.debug(f\"Generated safe public_id for empty base_name: {safe_id}\")\n            return safe_id\n\n        # Detect if the name contains Arabic characters\n        has_arabic = bool(re.search(r'[\\u0600-\\u06FF]', base_name))\n\n        if has_arabic:\n            logger.info(f\"Detected Arabic in contract name: {base_name}\")\n            english_name = translate_arabic_to_english(base_name)\n            clean_name = clean_filename(english_name)\n        else:\n            clean_name = clean_filename(base_name)\n\n        # Ensure the name is not too long\n        if len(clean_name) > max_length:\n            clean_name = clean_name[:max_length]\n\n        # Generate final public_id\n        if prefix:\n            safe_id = f\"{prefix}_{clean_name}_{uuid.uuid4().hex[:6]}\"\n        else:\n            safe_id = f\"{clean_name}_{uuid.uuid4().hex[:6]}\"\n\n        # Final safety check - remove any remaining problematic characters\n        safe_id = re.sub(r'[^a-zA-Z0-9_-]', '_', safe_id)\n\n        logger.debug(f\"Generated safe public_id: {safe_id} from base_name: {base_name}\")\n        return safe_id\n\n    except Exception as e:\n        logger.error(f\"Error generating safe public_id for '{base_name}': {e}\")\n        fallback_id = f\"{prefix}_{uuid.uuid4().hex[:8]}\"\n        return fallback_id","size_bytes":6032}},"version":1}